{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zuehlke HW Assignment 2\n",
    "\n",
    "## Done on a Mac, for a Mac\n",
    "\n",
    "\n",
    "=====DAMLAS ASSIGNMENT # 2=====\n",
    "\n",
    "Data Analytics and Machine Learning at Scale\n",
    "Target, Minneapolis\n",
    "\n",
    "ASSIGNMENT #2\n",
    "Version 2016-06-26\n",
    "Prepared by Dr. James G. Shanahan\n",
    "\n",
    "\n",
    "SCHEDULE: This Homework is due by  Friday, July 8, 2016 at 11 AM (Central Time).\n",
    "\n",
    "\n",
    "=== INSTRUCTIONS for SUBMISSIONS ===\n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "Prepare a single Jupyter note, please include questions, and question numbers in the questions and in the responses.\n",
    "Submit your homework notebook via the following form:\n",
    "\n",
    "   + http://goo.gl/forms/er3OFr5eCMWDngB72\n",
    "\n",
    "\n",
    "=== Other BACKGROUND INFORMATION\n",
    "This Homework, you will need to use a map-reduce framework to solve all  problems involving data. You can use MrJob or raw Hadoop mapreduce streaming.\n",
    "\n",
    "The following notebook serves as a useful Notebook to extend for this Homework (loaded with examples and boiler plate code)\n",
    "\n",
    "   +  https://www.dropbox.com/s/pjd6maluq4ogt7m/HW02-Supporting-Material.ipynb?dl=0\n",
    "\n",
    "Please refer to the following slides and reference material for more background\n",
    "\n",
    "\n",
    "Pattern in MapReduce : \n",
    "\n",
    "    + Lin, Jimmy, & Dyer, Chris. (2010). Data-intensive text processing with MapReduce. \n",
    "      San Rafael, CA: Morgan & Claypool Publishers. \n",
    "      Chapter 3\n",
    "      Book available here: https://lintool.github.io/MapReduceAlgorithms/MapReduce-book-final.pdf\n",
    "\n",
    "   + Chapter 2 in this book: Hadoop with MRJob (calculater salare average etc.) \n",
    "          -  https://www.dropbox.com/s/jd3z2s216p9kc1z/hadoop-with-python-MRJOB.pdf?dl=0\n",
    "          -  Source code: https://www.dropbox.com/sh/j8oettuxbgztk0p/AAAwq9PpEeByecDmaSNslnBPa?dl=0\n",
    "\n",
    "   + http://mrjob.readthedocs.io/en/latest/\n",
    "\n",
    "Counters in MRJob\n",
    "   + http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/5thl14n4pqvhzt5/Counter.ipynb \n",
    "\n",
    "KMeans Clustering (a form of Flat clustering)\n",
    "   + Slides:  https://www.dropbox.com/s/6ef1mlrqr3xpnw0/Lecture07.1-Clustering.pdf?dl=0\n",
    "   + http://nlp.stanford.edu/IR-book/pdf/16flat.pdf\n",
    "   + http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/dcw8evd9v0su3xu/K_Means_Unit_Test_Notebook.ipynb\n",
    "\n",
    "\n",
    "\n",
    "=== SERVER with MRJob installed\n",
    "\n",
    "http://ec2-52-201-222-181.compute-1.amazonaws.com:8000\n",
    "\n",
    " you will need a password to access.\n",
    "\n",
    "Dont forget to save your notebooks REGULARLY to your local laptops (as the cluster could crash without warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HW2.0\n",
    "## How do you merge  two sorted  lists/arrays of records of the form [key, value]? Where is this  used in Hadoop MapReduce?\n",
    "\n",
    "### _Assuming the lists are already sorted, compare the first element from each list and take the lesser element and add to a new sortedlist.  Then compare the larger of first elements with the next in the other, take the smaller and add to the sorted list. Repeat this process until both original lists have been completely parsed.  Here is some code to demonstrate further._\n",
    "\n",
    "```\n",
    "def mergeSortedLists(a,b):  #  Define class called mergeSortedLists with inputs a and b.  a and b are already sorted.\n",
    "    SortedList = []  # Instantiate SortedList empty list.\n",
    "    while a and b:   # Execute this loop until either a or b has been completely parsed.\n",
    "        if a[0] < b[0] :  # If the first element of a is less than b...\n",
    "            SortedList.append(a.pop(0)) # Take the first element of a and add it to SortedList.\n",
    "        else:             # If the first element of a is not less than b...\n",
    "            SortedList.append(b.pop(0)) # Take the first element of b and add it to SortedList.  Repeat this until both are empty.\n",
    "    return SortedList + a + b  Return SortedList + any remaining elements of a and b (the very last one done).\n",
    "\n",
    "```\n",
    "\n",
    "### _The process is done in the shuffle phase, between the mapper and reducer, to further simplify the reducer's load by providing fewer, sorted lists to perform the reducer tasks on._\n",
    "\n",
    "## What is  a combiner function in the context of Hadoop?  Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "\n",
    "### _A combiner is used between mapper and reducer to reduce the data transfer between mapper and reducer as a sort of \"pre reducer.\"  An example of where a combiner should be used (other than always) is with the Wordcount problem with a large dictionary.  Because there could be a large number of records coming from each mapper, the reducer could get overtaxed, which would lead to either a crash or a large amount of time to complete.  The combiner will act as an \"in-mapper reducer\" and consolidate the output from the mapper by combining similar keys.  This way, fewer records are passed to the reducer.  This speeds up the process and reduces the risk of system crash._\n",
    "\n",
    "## What is the Hadoop shuffle?\n",
    "### _The Hadoop shuffle is the process of moving key-value pairs from the a mapper to the reducer.  As data is being moved from mapper to reducer, it is being sorted to simplify the reducer's processing.  In the example of word count, a word appearing more than one time in the mapper's output to the reducer will be sorted together, so the reducer doesn't have to parse the entire input in order to aggregate._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.1\n",
    "## Counters as a debugging aid (and for getting work done, but please use sparingly as they are heavy)\n",
    "\n",
    "Consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "Use the following command to grab the file:\n",
    "\n",
    "    curl -L https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0 -o Consumer_Complaints.csv\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "User-defined Counters\n",
    "\n",
    "Now, let’s use MapReduce Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Pull data from the website and load into Consumer__Complaints csv, loaded into memory._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
      "100 48.5M  100 48.5M    0     0   500k      0  0:01:39  0:01:39 --:--:--  661k\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0 -o Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ComplaintCountByTypeHW21.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ComplaintCountByTypeHW21.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class ComplaintCountByType(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.decode('utf8', 'ignore')\n",
    "        \n",
    "        complaint_id, product, _ = line.split(',', 2)\n",
    "        del _\n",
    "\n",
    "        if product.lower() != 'product':\n",
    "            if product.lower() == 'debt collection':\n",
    "                self.increment_counter('complaint_type', 'debt collection', 1)  \n",
    "            elif product.lower() == 'mortgage':\n",
    "                self.increment_counter('complaint_type', 'mortgage', 1)        \n",
    "            else:\n",
    "                self.increment_counter('complaint_type', 'other', 1) \n",
    "            \n",
    "\n",
    "#The below combiners and reducers aren't used, but in order to keep programmatic consistency in alway using \n",
    "#mapper/reducer (always combiner!), I'm including them.\n",
    "\n",
    "    def combiner(self, complainttype, complaintcount):\n",
    "        yield complainttype, sum(complaintcount)\n",
    "\n",
    "    def reducer(self, complainttype, complaintcount):\n",
    "        yield complainttype, sum(complaintcount)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    ComplaintCountByType.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'complaint_type': {'debt collection': 44372, 'other': 142788, 'mortgage': 125752}}]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from ComplaintCountByTypeHW21 import ComplaintCountByType\n",
    "mr_job = ComplaintCountByType(args=['Consumer_Complaints.csv'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    print runner.counters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.2\n",
    "\n",
    "## Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!echo foo foo quux labs foo bar quux > WordLineH22.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2 Pt 1.1\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MRWordCountFunCallHW2211.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRWordCountFunCallHW2211.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    " \n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    " \n",
    "class MRWordCountFunCall(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        self.increment_counter('FunctionCalls', 'Mapper', 1)\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter('FunctionCalls', 'Reducer', 1)\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCountFunCall.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'FunctionCalls': {'Mapper': 1, 'Reducer': 4}}]\n",
      "\"bar\"\t1\n",
      "\"foo\"\t3\n",
      "\"labs\"\t1\n",
      "\"quux\"\t2\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from MRWordCountFunCallHW2211 import MRWordCountFunCall\n",
    "mr_job = MRWordCountFunCall(args=['WordLineH22.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    print runner.counters()\n",
    "    for line in runner.stream_output():\n",
    "        print line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you look at the txt file WordLineHW22's contents, 1 line = 1 mapper.  There are four distinct words, so each word is sent individually to the reducer from the mapper, therefore 4 reducer calls._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2 Pt 1.2\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many times the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MRWordCountFunCallHW2212.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRWordCountFunCallHW2212.py\n",
    "from mrjob.job import MRJob\n",
    "import re, string\n",
    " \n",
    "class MRWordCountFunCall(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        self.increment_counter('FunctionCalls', 'Num_mapper_calls', 1)\n",
    "        line = line.decode('utf8', 'ignore')\n",
    "        \n",
    "        complaint_id, complaint_type, sub_product, issue, _ = line.split(',', 4)\n",
    "\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        issue = regex.sub(' ', issue.lower())\n",
    "        issue = re.sub( '\\s+', ' ', issue)\n",
    "\n",
    "        issue = issue.split()\n",
    "        \n",
    "        try:\n",
    "            int(complaint_id)\n",
    "        except: \n",
    "            return\n",
    "        \n",
    "        del complaint_type, sub_product, _\n",
    "\n",
    "        \n",
    "        for word in issue:\n",
    "            yield word, 1\n",
    "        \n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter('FunctionCalls', 'Num_reducer_calls', 1)\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRWordCountFunCall.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'FunctionCalls': {'Num_mapper_calls': 312913, 'Num_reducer_calls': 172}}]\n",
      "\"a\"\t3503\n",
      "\"account\"\t20681\n",
      "\"acct\"\t163\n",
      "\"action\"\t2505\n",
      "\"advance\"\t240\n",
      "\"advertising\"\t1193\n",
      "\"amount\"\t98\n",
      "\"amt\"\t71\n",
      "\"an\"\t2505\n",
      "\"and\"\t16448\n",
      "\"application\"\t8868\n",
      "\"applied\"\t139\n",
      "\"apply\"\t118\n",
      "\"apr\"\t3431\n",
      "\"arbitration\"\t168\n",
      "\"are\"\t3821\n",
      "\"atm\"\t2422\n",
      "\"attempts\"\t11848\n",
      "\"available\"\t274\n",
      "\"balance\"\t597\n",
      "\"bank\"\t202\n",
      "\"bankruptcy\"\t222\n",
      "\"being\"\t5663\n",
      "\"billing\"\t8158\n",
      "\"by\"\t5663\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from MRWordCountFunCallHW2212 import MRWordCountFunCall\n",
    "mr_job = MRWordCountFunCall(args=['Consumer_Complaints.csv'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    counter = 1\n",
    "    print runner.counters()\n",
    "    for line in runner.stream_output():\n",
    "        print line.strip()\n",
    "        counter +=1\n",
    "        if counter > 25:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only printed 25 records to keep the finished notebook smaller.  I could remove the following code from the above runner to print the entire list.\n",
    "\n",
    "```\n",
    "        counter +=1\n",
    "        if counter > 25:\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of mapper calls: 312,913, Number of reducer calls: 172}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2 Pt 1.3\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MRWordCountFunCallHW2213.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRWordCountFunCallHW2213.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "import re, string\n",
    " \n",
    "class MRWordCountFunCallCombiner(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper,\n",
    "            ),\n",
    "            MRStep(   \n",
    "                combiner=self.combiner\n",
    "            ),\n",
    "            MRStep(\n",
    "                reducer=self.reducer  \n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        self.increment_counter('FunctionCalls', 'Num_mapper_calls', 1)\n",
    "        line = line.decode('utf8', 'ignore')\n",
    "        \n",
    "        complaint_id, complaint_type, sub_product, issue, _ = line.split(',', 4)\n",
    "\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        issue = regex.sub(' ', issue.lower())\n",
    "        issue = re.sub( '\\s+', ' ', issue)\n",
    "\n",
    "        issue = issue.split()\n",
    "        \n",
    "        try:\n",
    "            int(complaint_id)\n",
    "        except: \n",
    "            return\n",
    "        \n",
    "        del complaint_type, sub_product, _\n",
    "\n",
    "        for word in issue:\n",
    "            yield word, 1\n",
    "            \n",
    "    def combiner(self, word, counts):\n",
    "        self.increment_counter('FunctionCalls', 'Num_combiner_calls', 1)\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter('FunctionCalls', 'Num_reducer_calls', 1)\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRWordCountFunCallCombiner.run()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'FunctionCalls': {'Num_mapper_calls': 312913}}, {'FunctionCalls': {'Num_combiner_calls': 483}}, {'FunctionCalls': {'Num_reducer_calls': 172}}]\n",
      "\"a\"\t3503\n",
      "\"account\"\t20681\n",
      "\"acct\"\t163\n",
      "\"action\"\t2505\n",
      "\"advance\"\t240\n",
      "\"advertising\"\t1193\n",
      "\"amount\"\t98\n",
      "\"amt\"\t71\n",
      "\"an\"\t2505\n",
      "\"and\"\t16448\n",
      "\"application\"\t8868\n",
      "\"applied\"\t139\n",
      "\"apply\"\t118\n",
      "\"apr\"\t3431\n",
      "\"arbitration\"\t168\n",
      "\"are\"\t3821\n",
      "\"atm\"\t2422\n",
      "\"attempts\"\t11848\n",
      "\"available\"\t274\n",
      "\"balance\"\t597\n",
      "\"bank\"\t202\n",
      "\"bankruptcy\"\t222\n",
      "\"being\"\t5663\n",
      "\"billing\"\t8158\n",
      "\"by\"\t5663\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from MRWordCountFunCallHW2213 import MRWordCountFunCallCombiner\n",
    "mr_job = MRWordCountFunCallCombiner(args=['Consumer_Complaints.csv'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    counter = 1\n",
    "    print runner.counters()\n",
    "    for line in runner.stream_output():\n",
    "        print line.strip()\n",
    "        counter +=1\n",
    "        if counter > 25:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the value of your user defined Mapper Counter, combiner counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of mapper calls:  312,913.  Number of reducer calls:  172.  Number of combiner calls:  172\n",
    "\n",
    "I only printed 25 records to keep the finished notebook smaller.  I could remove the following code from the above runner to print the entire list.\n",
    "\n",
    "```\n",
    "        counter +=1\n",
    "        if counter > 25:\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.1\n",
    "Using a single reducer perform a sort of the words in decreasing order of word counts. Present the top 50 terms and their frequency. If there are ties please sort the tokens in alphanumeric/string order. \n",
    "\n",
    "HINT: You will need a second MRStep for the sort part. Step 1 will be the usual word count, while step 2 will be a sort step. Please use the Hadoop/MRJob framework to perform the sort. Please do NOT use any of the built-in sorts  from  python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MRWordCountFunCallMostFreqHW221.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRWordCountFunCallMostFreqHW221.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "import re, string\n",
    " \n",
    "class MRWordCountFunCallSorterMostFreq(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper\n",
    "               ,reducer=self.reducer\n",
    "            )\n",
    "             ,MRStep(\n",
    "                 mapper=self.mapper_sort\n",
    "                ,reducer=self.reducer_sort\n",
    "            )\n",
    "        ]\n",
    "\n",
    "#MRStep 1\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        self.increment_counter('FunctionCalls', 'Num_mapper_calls',1)\n",
    "        line = line.decode('utf8', 'ignore')\n",
    "        \n",
    "        complaint_id, complaint_type, sub_product, issue, _ = line.split(',', 4)\n",
    "\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        issue = regex.sub(' ', issue.lower())\n",
    "        issue = re.sub( '\\s+', ' ', issue)\n",
    "\n",
    "        issue = issue.split()\n",
    "        \n",
    "        try:\n",
    "            int(complaint_id)\n",
    "        except: \n",
    "            return\n",
    "        \n",
    "        del complaint_type, sub_product, _\n",
    "\n",
    "        for word in issue:\n",
    "            yield (word, 1)\n",
    "            \n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "#MRStep2\n",
    "    def mapper_sort(self, word, count):\n",
    "        yield (count-1e15, word), None\n",
    "        \n",
    "    def reducer_sort(self, countwords, _):\n",
    "        count, word = countwords\n",
    "        yield word, int(abs(count + 1e15))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRWordCountFunCallSorterMostFreq.run()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \"loan\"\t119630\n",
      "2 \"modification\"\t70487\n",
      "3 \"credit\"\t55251\n",
      "4 \"servicing\"\t36767\n",
      "5 \"report\"\t34903\n",
      "6 \"incorrect\"\t29133\n",
      "7 \"information\"\t29069\n",
      "8 \"on\"\t29069\n",
      "9 \"or\"\t22533\n",
      "10 \"account\"\t20681\n",
      "11 \"debt\"\t19309\n",
      "12 \"and\"\t16448\n",
      "13 \"opening\"\t16205\n",
      "14 \"club\"\t12545\n",
      "15 \"health\"\t12545\n",
      "16 \"not\"\t12353\n",
      "17 \"attempts\"\t11848\n",
      "18 \"collect\"\t11848\n",
      "19 \"cont\"\t11848\n",
      "20 \"d\"\t11848\n",
      "21 \"owed\"\t11848\n",
      "22 \"of\"\t10885\n",
      "23 \"my\"\t10731\n",
      "24 \"deposits\"\t10555\n",
      "25 \"withdrawals\"\t10555\n",
      "26 \"problems\"\t9484\n",
      "27 \"application\"\t8868\n",
      "28 \"to\"\t8401\n",
      "29 \"unable\"\t8178\n",
      "30 \"billing\"\t8158\n",
      "31 \"other\"\t7886\n",
      "32 \"disputes\"\t6938\n",
      "33 \"communication\"\t6920\n",
      "34 \"tactics\"\t6920\n",
      "35 \"reporting\"\t6559\n",
      "36 \"lease\"\t6337\n",
      "37 \"the\"\t6248\n",
      "38 \"being\"\t5663\n",
      "39 \"by\"\t5663\n",
      "40 \"caused\"\t5663\n",
      "41 \"funds\"\t5663\n",
      "42 \"low\"\t5663\n",
      "43 \"process\"\t5505\n",
      "44 \"disclosure\"\t5214\n",
      "45 \"verification\"\t5214\n",
      "46 \"managing\"\t5006\n",
      "47 \"company\"\t4858\n",
      "48 \"investigation\"\t4858\n",
      "49 \"s\"\t4858\n",
      "50 \"identity\"\t4729\n"
     ]
    }
   ],
   "source": [
    "from MRWordCountFunCallMostFreqHW221 import MRWordCountFunCallSorterMostFreq\n",
    "mr_job = MRWordCountFunCallSorterMostFreq(args=['Consumer_Complaints.csv', '--jobconf=\"mapred.reduce.tasks=1\"'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    counter = 1\n",
    "    for line in runner.stream_output():\n",
    "        print counter, line.strip()\n",
    "        counter += 1\n",
    "        if counter > 50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MRWordCountFunCallLeastFreqHW221.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRWordCountFunCallLeastFreqHW221.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "import re, string\n",
    " \n",
    "class MRWordCountFunCallSorterLeastFreq(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper\n",
    "                ,reducer=self.reducer\n",
    "            )\n",
    "             ,MRStep(\n",
    "                 mapper=self.mapper_sort\n",
    "                ,reducer=self.reducer_sort\n",
    "            )\n",
    "        ]\n",
    "\n",
    "#MRStep 1\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        self.increment_counter('FunctionCalls', 'Num_mapper_calls',1)\n",
    "        line = line.decode('utf8', 'ignore')\n",
    "        \n",
    "        complaint_id, complaint_type, sub_product, issue, _ = line.split(',', 4)\n",
    "\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        issue = regex.sub(' ', issue.lower())\n",
    "        issue = re.sub( '\\s+', ' ', issue)\n",
    "\n",
    "        issue = issue.split()\n",
    "        \n",
    "        try:\n",
    "            int(complaint_id)\n",
    "        except: \n",
    "            return\n",
    "        \n",
    "        del complaint_type, sub_product, _\n",
    "        \n",
    "        for word in issue:\n",
    "            if len(word)>1:\n",
    "                yield (word, 1)\n",
    "            \n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "# #MRStep2\n",
    "    def mapper_sort(self, word, counts):\n",
    "        new_key = '%010d'%int(counts) + word \n",
    "        yield new_key, (word, counts)\n",
    "        \n",
    "    def reducer_sort(self, new_key, values):\n",
    "        yield new_key,0\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRWordCountFunCallSorterLeastFreq.run()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ('disclosures', 64)\n",
      "2 ('missing', 64)\n",
      "3 ('amt', 71)\n",
      "4 ('day', 71)\n",
      "5 ('checks', 75)\n",
      "6 ('convenience', 75)\n",
      "7 ('credited', 92)\n",
      "8 ('payment', 92)\n",
      "9 ('amount', 98)\n",
      "10 ('apply', 118)\n"
     ]
    }
   ],
   "source": [
    "# #Ten Least Frequent\n",
    "            \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from MRWordCountFunCallLeastFreqHW221 import MRWordCountFunCallSorterLeastFreq\n",
    "mr_job = MRWordCountFunCallSorterLeastFreq(args=['Consumer_Complaints.csv', '--jobconf=\"mapred.reduce.tasks=1\"'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    counter = 1\n",
    "    for line in runner.stream_output():\n",
    "        keystring,dummy = line.split('\\t')\n",
    "        frequency = int(keystring[1:11])\n",
    "        word = keystring[11:-1]\n",
    "        print counter, (word, frequency)\n",
    "        counter += 1\n",
    "        if counter > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.2:   \n",
    "Repeat HW2.2.1 using 3 reducers. Use the same code as in HW2.2.1  with just one modification \n",
    "to the command line: just add --jobconf mapred.reduce.tasks=3 as see presented here: \n",
    "\n",
    "    python HW2.2WordCount.py --jobconf mapred.reduce.tasks=3 oneLinerTextFile.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Top 50 with 3 reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \"loan\"\t119630\n",
      "2 \"modification\"\t70487\n",
      "3 \"credit\"\t55251\n",
      "4 \"servicing\"\t36767\n",
      "5 \"report\"\t34903\n",
      "6 \"incorrect\"\t29133\n",
      "7 \"information\"\t29069\n",
      "8 \"on\"\t29069\n",
      "9 \"or\"\t22533\n",
      "10 \"account\"\t20681\n",
      "11 \"debt\"\t19309\n",
      "12 \"and\"\t16448\n",
      "13 \"opening\"\t16205\n",
      "14 \"club\"\t12545\n",
      "15 \"health\"\t12545\n",
      "16 \"not\"\t12353\n",
      "17 \"attempts\"\t11848\n",
      "18 \"collect\"\t11848\n",
      "19 \"cont\"\t11848\n",
      "20 \"d\"\t11848\n",
      "21 \"owed\"\t11848\n",
      "22 \"of\"\t10885\n",
      "23 \"my\"\t10731\n",
      "24 \"deposits\"\t10555\n",
      "25 \"withdrawals\"\t10555\n",
      "26 \"problems\"\t9484\n",
      "27 \"application\"\t8868\n",
      "28 \"to\"\t8401\n",
      "29 \"unable\"\t8178\n",
      "30 \"billing\"\t8158\n",
      "31 \"other\"\t7886\n",
      "32 \"disputes\"\t6938\n",
      "33 \"communication\"\t6920\n",
      "34 \"tactics\"\t6920\n",
      "35 \"reporting\"\t6559\n",
      "36 \"lease\"\t6337\n",
      "37 \"the\"\t6248\n",
      "38 \"being\"\t5663\n",
      "39 \"by\"\t5663\n",
      "40 \"caused\"\t5663\n",
      "41 \"funds\"\t5663\n",
      "42 \"low\"\t5663\n",
      "43 \"process\"\t5505\n",
      "44 \"disclosure\"\t5214\n",
      "45 \"verification\"\t5214\n",
      "46 \"managing\"\t5006\n",
      "47 \"company\"\t4858\n",
      "48 \"investigation\"\t4858\n",
      "49 \"s\"\t4858\n",
      "50 \"identity\"\t4729\n"
     ]
    }
   ],
   "source": [
    "from MRWordCountFunCallMostFreqHW221 import MRWordCountFunCallSorterMostFreq\n",
    "mr_job = MRWordCountFunCallSorterMostFreq(args=['Consumer_Complaints.csv', '--jobconf=\"mapred.reduce.tasks=3\"'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    counter = 1\n",
    "    for line in runner.stream_output():\n",
    "        print counter, line.strip()\n",
    "        counter += 1\n",
    "        if counter > 50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom 10 with 3 reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ('disclosures', 64)\n",
      "2 ('missing', 64)\n",
      "3 ('amt', 71)\n",
      "4 ('day', 71)\n",
      "5 ('checks', 75)\n",
      "6 ('convenience', 75)\n",
      "7 ('credited', 92)\n",
      "8 ('payment', 92)\n",
      "9 ('amount', 98)\n",
      "10 ('apply', 118)\n"
     ]
    }
   ],
   "source": [
    "# #Ten Least Frequent\n",
    "            \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from MRWordCountFunCallLeastFreqHW221 import MRWordCountFunCallSorterLeastFreq\n",
    "mr_job = MRWordCountFunCallSorterLeastFreq(args=['Consumer_Complaints.csv', '--jobconf=\"mapred.reduce.tasks=3\"'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    counter = 1\n",
    "    for line in runner.stream_output():\n",
    "        keystring,dummy = line.split('\\t')\n",
    "        frequency = int(keystring[1:11])\n",
    "        word = keystring[11:-1]\n",
    "        print counter, (word, frequency)\n",
    "        counter += 1\n",
    "        if counter > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe what you see. Is this correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see no difference in the output.  Running my code works the same with 1 or 3 reducers, which is to be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2.2.3: \n",
    "## Optional; we will cover this in class\n",
    "\n",
    "I've already solved the \"total sort\" issue in my code above :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.3: Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "  \n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the data from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
      "100 3377k  100 3377k    0     0   370k      0  0:00:09  0:00:09 --:--:--  790k\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0 -o ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some exploratory data analysis of this dataset guided by the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without using MapReduce... How many unique items are available from this supplier?  Assuming the full catalog has been bought within the scope of the ProductPurchaseData.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This supplier has 12,592 unique products.\n"
     ]
    }
   ],
   "source": [
    "purchasedata = open('ProductPurchaseData.txt',\"r+\")\n",
    "\n",
    "products = []\n",
    "\n",
    "for session in purchasedata:\n",
    "    prodID = session.split()\n",
    "    for product in prodID:\n",
    "        if product in products:\n",
    "            continue\n",
    "        else:\n",
    "            products.append(product)\n",
    "            \n",
    "print \"This supplier has \" + str(\"{:,}\".format(len(products))) + \" unique products.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.3 Part 1\n",
    "## Using a single reducer, report your findings.  Use Hadoop MapReduce on the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What is the number of unique products?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing UniqueProductsHW231.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile UniqueProductsHW231.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re, string\n",
    "\n",
    "class MRJobUniqueProducts(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper,\n",
    "                reducer=self.reducer  \n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        session = regex.sub(' ', line.lower())\n",
    "        session = re.sub( '\\s+', ' ', session)\n",
    "        session = session.split()\n",
    "        \n",
    "        for prod in session:\n",
    "            yield (len(prod), prod)\n",
    "    \n",
    "    def combiner(self, key, values):\n",
    "        yield key, sum(values)\n",
    "            \n",
    "    def reducer(self, key, values):\n",
    "        yield \"Number of Unique Products\", len(set(values))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJobUniqueProducts.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Number of Unique Products\"\t12592\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from UniqueProductsHW231 import MRJobUniqueProducts\n",
    "\n",
    "mr_job = MRJobUniqueProducts(args=['ProductPurchaseData.txt', '--jobconf=\"mapred.reduce.tasks=1\"']) \n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches the number from above, so I know this is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What is the largest basket?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing LargestBasketHW231.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile LargestBasketHW231.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re, string\n",
    "\n",
    "class LargestBasket(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.mapper_sort,\n",
    "                reducer=self.reducer_sort\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        session = regex.sub(' ', line.lower())\n",
    "        session = re.sub( '\\s+', ' ', session)\n",
    "        session = session.split()\n",
    "        basket_size = len(session)\n",
    "\n",
    "        yield (\"Dummy\", basket_size)\n",
    "        \n",
    "    def mapper_sort(self, basket, basket_size):\n",
    "        yield ((basket_size-1e15, basket), None)\n",
    "    \n",
    "    def reducer_sort(self, sizebasket, _):\n",
    "        size, basket = sizebasket\n",
    "        yield \"Largest Basket Size\", int(abs(size + 1e15))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    LargestBasket.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Largest Basket Size\"\t37\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from LargestBasketHW231 import LargestBasket\n",
    "mr_job = LargestBasket(args=['ProductPurchaseData.txt', '--jobconf=\"mapred.reduce.tasks=1\"']) \n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    for line in runner.stream_output():\n",
    "        print line.strip()\n",
    "        count += 1\n",
    "        if count > 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What are the top 50 most frequently purchased items and their frequency?  (Break ties by sorting products alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Top50ProductsHW231.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Top50ProductsHW231.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re, string\n",
    "\n",
    "class Top50Products(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper,\n",
    "                combiner=self.combiner,\n",
    "                reducer=self.reducer\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.mapper_sort,\n",
    "                reducer=self.reducer_sort\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        session = regex.sub(' ', line.lower())\n",
    "        session = re.sub( '\\s+', ' ', session)\n",
    "        session = session.split()\n",
    "        basket_size = len(session)\n",
    "        \n",
    "        for item in session:\n",
    "            yield item, 1\n",
    "        \n",
    "    def combiner(self, item, counts):\n",
    "        yield item, sum(counts)\n",
    "        \n",
    "    def reducer(self, item, counts):\n",
    "        yield item, sum(counts)\n",
    "        \n",
    "#MRStep2\n",
    "    def mapper_sort(self, word, count):\n",
    "        yield (count-1e15, word), None\n",
    "        \n",
    "    def reducer_sort(self, countwords, _):\n",
    "        count, word = countwords\n",
    "        yield word.upper(), int(abs(count + 1e15))\n",
    "                        \n",
    "if __name__ == '__main__':\n",
    "    Top50Products.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DAI62779\"\t6667\n",
      "\"FRO40251\"\t3881\n",
      "\"ELE17451\"\t3875\n",
      "\"GRO73461\"\t3602\n",
      "\"SNA80324\"\t3044\n",
      "\"ELE32164\"\t2851\n",
      "\"DAI75645\"\t2736\n",
      "\"SNA45677\"\t2455\n",
      "\"FRO31317\"\t2330\n",
      "\"DAI85309\"\t2293\n",
      "\"ELE26917\"\t2292\n",
      "\"FRO80039\"\t2233\n",
      "\"GRO21487\"\t2115\n",
      "\"SNA99873\"\t2083\n",
      "\"GRO59710\"\t2004\n",
      "\"GRO71621\"\t1920\n",
      "\"FRO85978\"\t1918\n",
      "\"GRO30386\"\t1840\n",
      "\"ELE74009\"\t1816\n",
      "\"GRO56726\"\t1784\n",
      "\"DAI63921\"\t1773\n",
      "\"GRO46854\"\t1756\n",
      "\"ELE66600\"\t1713\n",
      "\"DAI83733\"\t1712\n",
      "\"FRO32293\"\t1702\n",
      "\"ELE66810\"\t1697\n",
      "\"SNA55762\"\t1646\n",
      "\"DAI22177\"\t1627\n",
      "\"FRO78087\"\t1531\n",
      "\"ELE99737\"\t1516\n",
      "\"ELE34057\"\t1489\n",
      "\"GRO94758\"\t1489\n",
      "\"FRO35904\"\t1436\n",
      "\"FRO53271\"\t1420\n",
      "\"SNA93860\"\t1407\n",
      "\"SNA90094\"\t1390\n",
      "\"GRO38814\"\t1352\n",
      "\"ELE56788\"\t1345\n",
      "\"GRO61133\"\t1321\n",
      "\"DAI88807\"\t1316\n",
      "\"ELE74482\"\t1316\n",
      "\"ELE59935\"\t1311\n",
      "\"SNA96271\"\t1295\n",
      "\"DAI43223\"\t1290\n",
      "\"ELE91337\"\t1289\n",
      "\"GRO15017\"\t1275\n",
      "\"DAI31081\"\t1261\n",
      "\"GRO81087\"\t1220\n",
      "\"DAI22896\"\t1219\n",
      "\"GRO85051\"\t1214\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from Top50ProductsHW231 import Top50Products\n",
    "mr_job = Top50Products(args=['ProductPurchaseData.txt', '--jobconf=\"mapred.reduce.tasks=1\"'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    for line in runner.stream_output():\n",
    "        print line.strip()\n",
    "        count += 1\n",
    "        if count > 49:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items, and their frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What is the number of unique products?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Number of Unique Products\"\t12592\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from UniqueProductsHW231 import MRJobUniqueProducts\n",
    "\n",
    "mr_job = MRJobUniqueProducts(args=['ProductPurchaseData.txt', '--jobconf=\"mapred.reduce.tasks=2\"']) \n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What is the largest basket?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Largest Basket Size\"\t37\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from LargestBasketHW231 import LargestBasket\n",
    "mr_job = LargestBasket(args=['ProductPurchaseData.txt', '--jobconf=\"mapred.reduce.tasks=2\"']) \n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    for line in runner.stream_output():\n",
    "        print line.strip()\n",
    "        count += 1\n",
    "        if count > 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What are the top 50 most frequently purchased items and their frequency?  (Break ties by sorting products alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \"DAI62779\"\t6667\n",
      "2 \"FRO40251\"\t3881\n",
      "3 \"ELE17451\"\t3875\n",
      "4 \"GRO73461\"\t3602\n",
      "5 \"SNA80324\"\t3044\n",
      "6 \"ELE32164\"\t2851\n",
      "7 \"DAI75645\"\t2736\n",
      "8 \"SNA45677\"\t2455\n",
      "9 \"FRO31317\"\t2330\n",
      "10 \"DAI85309\"\t2293\n",
      "11 \"ELE26917\"\t2292\n",
      "12 \"FRO80039\"\t2233\n",
      "13 \"GRO21487\"\t2115\n",
      "14 \"SNA99873\"\t2083\n",
      "15 \"GRO59710\"\t2004\n",
      "16 \"GRO71621\"\t1920\n",
      "17 \"FRO85978\"\t1918\n",
      "18 \"GRO30386\"\t1840\n",
      "19 \"ELE74009\"\t1816\n",
      "20 \"GRO56726\"\t1784\n",
      "21 \"DAI63921\"\t1773\n",
      "22 \"GRO46854\"\t1756\n",
      "23 \"ELE66600\"\t1713\n",
      "24 \"DAI83733\"\t1712\n",
      "25 \"FRO32293\"\t1702\n",
      "26 \"ELE66810\"\t1697\n",
      "27 \"SNA55762\"\t1646\n",
      "28 \"DAI22177\"\t1627\n",
      "29 \"FRO78087\"\t1531\n",
      "30 \"ELE99737\"\t1516\n",
      "31 \"ELE34057\"\t1489\n",
      "32 \"GRO94758\"\t1489\n",
      "33 \"FRO35904\"\t1436\n",
      "34 \"FRO53271\"\t1420\n",
      "35 \"SNA93860\"\t1407\n",
      "36 \"SNA90094\"\t1390\n",
      "37 \"GRO38814\"\t1352\n",
      "38 \"ELE56788\"\t1345\n",
      "39 \"GRO61133\"\t1321\n",
      "40 \"DAI88807\"\t1316\n",
      "41 \"ELE74482\"\t1316\n",
      "42 \"ELE59935\"\t1311\n",
      "43 \"SNA96271\"\t1295\n",
      "44 \"DAI43223\"\t1290\n",
      "45 \"ELE91337\"\t1289\n",
      "46 \"GRO15017\"\t1275\n",
      "47 \"DAI31081\"\t1261\n",
      "48 \"GRO81087\"\t1220\n",
      "49 \"DAI22896\"\t1219\n",
      "50 \"GRO85051\"\t1214\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from Top50ProductsHW231 import Top50Products\n",
    "mr_job = Top50Products(args=['ProductPurchaseData.txt', '--jobconf=\"mapred.reduce.tasks=2\"'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    counter = 1\n",
    "    for line in runner.stream_output():\n",
    "        print counter, line.strip()\n",
    "        counter += 1\n",
    "        if counter > 50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.4\n",
    "## (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "From a data mining perspective (and the aPriori algorihtm in particular), Support and Confidence are defined as follows:\n",
    "\n",
    "       SUPPORT\n",
    "       In data mining, the support value of X (where X is a collection of cooccurring items sometimes referred to as \n",
    "       an item-set. E.g., a basket or subset of a basket) with respect to T  (a transaction database where each \n",
    "       row is a transaction such as a basket of items that have been purchased)  is defined as the proportion \n",
    "       of transactions in the  database which contains  the item-set X. (a relative frequency of sorts) \n",
    "\n",
    "       CONFIDENCE \n",
    "       The confidence value of a rule, X ==>  Y (where X is a collection of cooccurring items and Y is generally \n",
    "       a single item. E.g., If Diapers and Beer then Cigars were also purchased), with respect to a set of transactions T, is the \n",
    "       proportion of the transactions that contains X which also contains Y. (Think of it as  tgePr(Y|X) )\n",
    "\n",
    "       The pairs/stripes algorithm returns cooccurrence information that can be used directly to  calculate the confidence and support. \n",
    "       Note that confidence for pair X ==>  Y will  differ from the relative frequency that results from stripes when X occurs by itself in transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.4.1\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing PairsHW241.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PairsHW241.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re, string\n",
    "\n",
    "class Pairs(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        self.increment_counter('FunctionCalls', 'Mapper', 1)\n",
    "        \n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        session = regex.sub(' ', line.lower())\n",
    "        session = re.sub( '\\s+', ' ', session)\n",
    "        session = list(set(session.split()))\n",
    "        session.sort()\n",
    "        \n",
    "        for i, item1 in enumerate(session):\n",
    "            for item2 in session[i + 1:]:\n",
    "                yield (item1, item2), 1\n",
    "            \n",
    "    def combiner(self, pair, counts):\n",
    "        self.increment_counter('FunctionCalls', 'Combiner', 1)\n",
    "        yield pair, sum(counts)\n",
    "        \n",
    "    def reducer(self, pair, counts):\n",
    "        self.increment_counter('FunctionCalls', 'Reducer', 1)\n",
    "                               \n",
    "        count = sum(counts)\n",
    "        if count > 99:\n",
    "            yield pair, count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Pairs.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"dai16732\", \"fro78087\"]\t106\n",
      "[\"dai18527\", \"sna44451\"]\t102\n",
      "[\"dai22177\", \"dai31081\"]\t127\n",
      "[\"dai22177\", \"dai62779\"]\t382\n",
      "[\"dai22177\", \"dai63921\"]\t136\n",
      "[\"dai22177\", \"dai75645\"]\t123\n",
      "[\"dai22177\", \"dai83733\"]\t126\n",
      "[\"dai22177\", \"dai85309\"]\t172\n",
      "[\"dai22177\", \"ele17451\"]\t203\n",
      "[\"dai22177\", \"ele26917\"]\t134\n",
      "[\"dai22177\", \"ele32164\"]\t155\n",
      "[\"dai22177\", \"ele34057\"]\t107\n",
      "[\"dai22177\", \"ele56788\"]\t134\n",
      "[\"dai22177\", \"ele66600\"]\t101\n",
      "[\"dai22177\", \"ele66810\"]\t105\n",
      "[\"dai22177\", \"ele74009\"]\t108\n",
      "[\"dai22177\", \"ele91337\"]\t150\n",
      "[\"dai22177\", \"fro31317\"]\t160\n",
      "[\"dai22177\", \"fro32293\"]\t128\n",
      "[\"dai22177\", \"fro40251\"]\t181\n",
      "[\"dai22177\", \"fro66272\"]\t130\n",
      "[\"dai22177\", \"fro78087\"]\t107\n",
      "[\"dai22177\", \"fro80039\"]\t152\n",
      "[\"dai22177\", \"fro85978\"]\t156\n",
      "[\"dai22177\", \"gro21487\"]\t122\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PairsHW241 import Pairs\n",
    "mr_job = Pairs(args=['ProductPurchaseData.txt']) \n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    counter = 1\n",
    "    for line in runner.stream_output():\n",
    "        print line.strip()\n",
    "        counter +=1\n",
    "        if counter > 25:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only printed 25 records to keep the finished notebook smaller.  I could remove the following code from the above runner to print the entire list.\n",
    "\n",
    "```\n",
    "        counter +=1\n",
    "        if counter > 25:\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.4.2\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support  (OPTIONAL Feel free to add in confidence level also)\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing PairsTop50HW242.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PairsTop50HW242.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re, string\n",
    "\n",
    "pairdata = {}\n",
    "\n",
    "class Top50Pairs(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper\n",
    "                ,combiner=self.combiner\n",
    "                ,reducer=self.reducer\n",
    "            )\n",
    "            ,MRStep(\n",
    "                mapper=self.mapper_sort\n",
    "                ,reducer=self.reducer_sort\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        session = regex.sub(' ', line.lower())\n",
    "        session = re.sub( '\\s+', ' ', session)\n",
    "        session = list(set(session.split()))\n",
    "        session.sort()\n",
    "        \n",
    "        for i, item1 in enumerate(session):\n",
    "            for item2 in session[i + 1:]:\n",
    "                yield (item1, item2), 1\n",
    "        yield (\"\", \"total\"), 1\n",
    "\n",
    "    def combiner(self, pair, counts):\n",
    "        yield pair, sum(counts)\n",
    "        \n",
    "    def reducer(self, pair, counts):\n",
    "        count = sum(counts)\n",
    "        \n",
    "        if len(pair[0]) < 1:\n",
    "            pairdata[pair[1]] = count\n",
    "            return\n",
    "        if count > 99:\n",
    "            yield pair, count\n",
    "\n",
    "    def mapper_sort(self, pair, count):\n",
    "\n",
    "        yield (1e15 - count, pair), None      \n",
    "        \n",
    "    def reducer_sort(self, countpair, _):\n",
    "        total = float(pairdata[\"total\"])\n",
    "        adjcount, pair = countpair\n",
    "        frequency = int(abs(adjcount - 1e15))\n",
    "        rel_freq = frequency / total\n",
    "        frequencies = (frequency, rel_freq)\n",
    "        yield pair, frequencies\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Top50Pairs.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 DAI62779 ELE17451 1592 0.0511880646925\n",
      "2 FRO40251 SNA80324 1412 0.0454004694383\n",
      "3 DAI75645 FRO40251 1254 0.0403202469374\n",
      "4 FRO40251 GRO85051 1213 0.0390019613517\n",
      "5 DAI62779 GRO73461 1139 0.0366226166361\n",
      "6 DAI75645 SNA80324 1130 0.0363332368734\n",
      "7 DAI62779 FRO40251 1070 0.0344040384554\n",
      "8 DAI62779 SNA80324 923 0.0296775023311\n",
      "9 DAI62779 DAI85309 918 0.0295167357963\n",
      "10 ELE32164 GRO59710 911 0.0292916626475\n",
      "11 DAI62779 DAI75645 882 0.0283592167454\n",
      "12 FRO40251 GRO73461 882 0.0283592167454\n",
      "13 DAI62779 ELE92920 877 0.0281984502106\n",
      "14 FRO40251 FRO92469 835 0.026848011318\n",
      "15 DAI62779 ELE32164 832 0.0267515513971\n",
      "16 DAI75645 GRO73461 712 0.0228931545609\n",
      "17 DAI43223 ELE32164 711 0.022861001254\n",
      "18 DAI62779 GRO30386 709 0.02279669464\n",
      "19 ELE17451 FRO40251 697 0.0224108549564\n",
      "20 DAI85309 ELE99737 659 0.0211890292917\n",
      "21 DAI62779 ELE26917 650 0.020899649529\n",
      "22 GRO21487 GRO73461 631 0.0202887366966\n",
      "23 DAI62779 SNA45677 604 0.0194205974084\n",
      "24 ELE17451 SNA80324 597 0.0191955242597\n",
      "25 DAI62779 GRO71621 595 0.0191312176457\n",
      "26 DAI62779 SNA55762 593 0.0190669110318\n",
      "27 DAI62779 DAI83733 586 0.018841837883\n",
      "28 ELE17451 GRO73461 580 0.0186489180412\n",
      "29 GRO73461 SNA80324 562 0.0180701585158\n",
      "30 DAI62779 GRO59710 561 0.0180380052088\n",
      "31 DAI62779 FRO80039 550 0.0176843188322\n",
      "32 DAI75645 ELE17451 547 0.0175878589113\n",
      "33 DAI62779 SNA93860 537 0.0172663258416\n",
      "34 DAI55148 DAI62779 526 0.016912639465\n",
      "35 DAI43223 GRO59710 512 0.0164624931674\n",
      "36 ELE17451 ELE32164 511 0.0164303398605\n",
      "37 DAI62779 SNA18336 506 0.0162695733256\n",
      "38 ELE32164 GRO73461 486 0.0156265071863\n",
      "39 DAI62779 FRO78087 482 0.0154978939584\n",
      "40 DAI85309 ELE17451 482 0.0154978939584\n",
      "41 DAI62779 GRO94758 479 0.0154014340375\n",
      "42 DAI62779 GRO21487 471 0.0151442075817\n",
      "43 GRO85051 SNA80324 471 0.0151442075817\n",
      "44 ELE17451 GRO30386 468 0.0150477476608\n",
      "45 FRO85978 SNA95666 463 0.014886981126\n",
      "46 DAI62779 FRO19221 462 0.014854827819\n",
      "47 DAI62779 GRO46854 461 0.0148226745121\n",
      "48 DAI43223 DAI62779 459 0.0147583678981\n",
      "49 ELE92920 SNA18336 455 0.0146297546703\n",
      "50 DAI88079 FRO40251 446 0.0143403749076\n"
     ]
    }
   ],
   "source": [
    "from PairsTop50HW242 import Top50Pairs\n",
    "mr_job = Top50Pairs(args=['ProductPurchaseData.txt']) \n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    counter = 1      \n",
    "    for line in runner.stream_output():\n",
    "        pair, frequencies =  mr_job.parse_output_line(line)\n",
    "        print counter, pair[0].upper(), pair[1].upper(), frequencies[0], frequencies[1]\n",
    "        counter += 1\n",
    "        if counter > 50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.4.3\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing PairsHW241_runnertimer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PairsHW241_runnertimer.py\n",
    "from PairsHW241 import Pairs\n",
    "mr_job = Pairs(args=['ProductPurchaseData.txt', '--jobconf=\"mapred.map.tasks=2\"', '--jobconf=\"mapred.reduce.tasks=2\"']) \n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    print runner.counters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'FunctionCalls': {'Mapper': 31101, 'Reducer': 877095, 'Combiner': 1026707}}]\n",
      "CPU times: user 402 ms, sys: 127 ms, total: 529 ms\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%time !python PairsHW241_runnertimer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine used was a quad core Mac, running on OS X.  Mapper/Reducer count set to 2.  Number of Mapper, Reducer, Combiner, MapperSort and ReducerSort calls in the results above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.5\n",
    "## Stripes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.5.1\n",
    "Repeat 2.4.1 using the stripes design pattern for finding cooccuring pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing StripesHW251.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile StripesHW251.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re, string\n",
    "from collections import Counter\n",
    "\n",
    "stripedata = {}\n",
    "\n",
    "class Stripes(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper,\n",
    "                combiner=self.combiner,\n",
    "                reducer=self.reducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        self.increment_counter('FunctionCalls', 'Mapper', 1)\n",
    "        \n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        session = regex.sub(' ', line.lower())\n",
    "        session = re.sub( '\\s+', ' ', session)\n",
    "        session = list(set(session.split()))\n",
    "        session.sort()\n",
    "        \n",
    "        for i, item1 in enumerate(session):\n",
    "            counts = Counter()\n",
    "            for item2 in session[i + 1:]:\n",
    "                counts[item2] += 1\n",
    "\n",
    "            yield item1, counts\n",
    "            \n",
    "        yield \"total\", {'total': 1}\n",
    "\n",
    "    def combiner(self, item, counters):\n",
    "        self.increment_counter('FunctionCalls', 'Combiner', 1)\n",
    "        counts = Counter()\n",
    "        for c in counters: \n",
    "            for item2, count in c.items():\n",
    "                counts[item2] += count\n",
    "                \n",
    "        yield item, counts\n",
    "        \n",
    "    def reducer(self, item, counters):\n",
    "        self.increment_counter('FunctionCalls', 'Reducer', 1)\n",
    "        \n",
    "        counts = Counter()\n",
    "        for c in counters: \n",
    "            for item2, count in c.items():\n",
    "                counts[item2] += count\n",
    "        \n",
    "        if item == \"total\":\n",
    "            stripedata[item] = counts[item]\n",
    "            return\n",
    "        \n",
    "        for item2, count in counts.items():\n",
    "            if count > 99:\n",
    "                yield (item, item2), count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Stripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"dai16732\", \"fro78087\"]\t106\n",
      "[\"dai18527\", \"sna44451\"]\t102\n",
      "[\"dai22177\", \"gro21487\"]\t122\n",
      "[\"dai22177\", \"sna55762\"]\t102\n",
      "[\"dai22177\", \"fro80039\"]\t152\n",
      "[\"dai22177\", \"dai85309\"]\t172\n",
      "[\"dai22177\", \"ele91337\"]\t150\n",
      "[\"dai22177\", \"dai62779\"]\t382\n",
      "[\"dai22177\", \"ele17451\"]\t203\n",
      "[\"dai22177\", \"dai75645\"]\t123\n",
      "[\"dai22177\", \"gro73461\"]\t248\n",
      "[\"dai22177\", \"ele32164\"]\t155\n",
      "[\"dai22177\", \"sna80324\"]\t140\n",
      "[\"dai22177\", \"gro71621\"]\t132\n",
      "[\"dai22177\", \"dai63921\"]\t136\n",
      "[\"dai22177\", \"fro78087\"]\t107\n",
      "[\"dai22177\", \"gro30386\"]\t106\n",
      "[\"dai22177\", \"fro31317\"]\t160\n",
      "[\"dai22177\", \"dai31081\"]\t127\n",
      "[\"dai22177\", \"ele74009\"]\t108\n",
      "[\"dai22177\", \"gro46854\"]\t160\n",
      "[\"dai22177\", \"fro85978\"]\t156\n",
      "[\"dai22177\", \"gro59710\"]\t120\n",
      "[\"dai22177\", \"fro40251\"]\t181\n",
      "[\"dai22177\", \"fro66272\"]\t130\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from StripesHW251 import Stripes\n",
    "mr_job = Stripes(args=['ProductPurchaseData.txt']) \n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    counter = 1\n",
    "    for line in runner.stream_output():\n",
    "        print line.strip()\n",
    "        counter +=1\n",
    "        if counter > 25:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only printed 25 records to keep the finished notebook smaller.  I could remove the following code from the above runner to print the entire list.\n",
    "\n",
    "```\n",
    "        counter +=1\n",
    "        if counter > 25:\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.5.2\n",
    "Stripes top 50.\n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support  (OPTIONAL Feel free to add in confidence level also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing StripesTop50HW252.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile StripesTop50HW252.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re, string\n",
    "from collections import Counter\n",
    "\n",
    "stripedata = {}\n",
    "\n",
    "class StripesTop50(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper,\n",
    "                combiner=self.combiner,\n",
    "                reducer=self.reducer\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.mapper_sort,\n",
    "                reducer=self.reducer_sort\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        self.increment_counter('FunctionCalls', 'Mapper', 1)\n",
    "        \n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        session = regex.sub(' ', line.lower())\n",
    "        session = re.sub( '\\s+', ' ', session)\n",
    "        session = list(set(session.split()))\n",
    "        session.sort()\n",
    "        \n",
    "        for i, item1 in enumerate(session):\n",
    "            counts = Counter()\n",
    "            for item2 in session[i + 1:]:\n",
    "                counts[item2] += 1\n",
    "\n",
    "            yield item1, counts\n",
    "            \n",
    "        yield \"total\", {'total': 1}\n",
    "\n",
    "    def combiner(self, item, counters):\n",
    "        self.increment_counter('FunctionCalls', 'Combiner', 1)\n",
    "        \n",
    "        counts = Counter()\n",
    "        for c in counters: \n",
    "            for item2, count in c.items():\n",
    "                counts[item2] += count\n",
    "        yield item, counts\n",
    "        \n",
    "    def reducer(self, item, counters):\n",
    "        self.increment_counter('FunctionCalls', 'Reducer', 1)\n",
    "        \n",
    "        counts = Counter()\n",
    "        for c in counters: \n",
    "            for item2, count in c.items():\n",
    "                counts[item2] += count\n",
    "        \n",
    "        if item == \"total\":\n",
    "            stripedata[item] = counts[item]\n",
    "            return\n",
    "        \n",
    "        for item2, count in counts.items():\n",
    "            if count > 99:\n",
    "                yield (item, item2), count\n",
    "                \n",
    "    def mapper_sort(self, pair, count):\n",
    "        self.increment_counter('FunctionCalls', 'MapperSort', 1)\n",
    "        \n",
    "        yield (1e15 - count, pair), None      \n",
    "        \n",
    "    def reducer_sort(self, countpair, _):\n",
    "        self.increment_counter('FunctionCalls', 'Reducer', 1)\n",
    "\n",
    "        total = float(stripedata[\"total\"])\n",
    "        adjcount, pair = countpair\n",
    "        frequency = int(abs(adjcount - 1e15))\n",
    "        rel_freq = frequency / total\n",
    "        frequencies = (frequency, rel_freq)\n",
    "        yield pair, frequencies\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    StripesTop50.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 DAI62779 ELE17451 1592 0.0511880646925\n",
      "2 FRO40251 SNA80324 1412 0.0454004694383\n",
      "3 DAI75645 FRO40251 1254 0.0403202469374\n",
      "4 FRO40251 GRO85051 1213 0.0390019613517\n",
      "5 DAI62779 GRO73461 1139 0.0366226166361\n",
      "6 DAI75645 SNA80324 1130 0.0363332368734\n",
      "7 DAI62779 FRO40251 1070 0.0344040384554\n",
      "8 DAI62779 SNA80324 923 0.0296775023311\n",
      "9 DAI62779 DAI85309 918 0.0295167357963\n",
      "10 ELE32164 GRO59710 911 0.0292916626475\n",
      "11 DAI62779 DAI75645 882 0.0283592167454\n",
      "12 FRO40251 GRO73461 882 0.0283592167454\n",
      "13 DAI62779 ELE92920 877 0.0281984502106\n",
      "14 FRO40251 FRO92469 835 0.026848011318\n",
      "15 DAI62779 ELE32164 832 0.0267515513971\n",
      "16 DAI75645 GRO73461 712 0.0228931545609\n",
      "17 DAI43223 ELE32164 711 0.022861001254\n",
      "18 DAI62779 GRO30386 709 0.02279669464\n",
      "19 ELE17451 FRO40251 697 0.0224108549564\n",
      "20 DAI85309 ELE99737 659 0.0211890292917\n",
      "21 DAI62779 ELE26917 650 0.020899649529\n",
      "22 GRO21487 GRO73461 631 0.0202887366966\n",
      "23 DAI62779 SNA45677 604 0.0194205974084\n",
      "24 ELE17451 SNA80324 597 0.0191955242597\n",
      "25 DAI62779 GRO71621 595 0.0191312176457\n",
      "26 DAI62779 SNA55762 593 0.0190669110318\n",
      "27 DAI62779 DAI83733 586 0.018841837883\n",
      "28 ELE17451 GRO73461 580 0.0186489180412\n",
      "29 GRO73461 SNA80324 562 0.0180701585158\n",
      "30 DAI62779 GRO59710 561 0.0180380052088\n",
      "31 DAI62779 FRO80039 550 0.0176843188322\n",
      "32 DAI75645 ELE17451 547 0.0175878589113\n",
      "33 DAI62779 SNA93860 537 0.0172663258416\n",
      "34 DAI55148 DAI62779 526 0.016912639465\n",
      "35 DAI43223 GRO59710 512 0.0164624931674\n",
      "36 ELE17451 ELE32164 511 0.0164303398605\n",
      "37 DAI62779 SNA18336 506 0.0162695733256\n",
      "38 ELE32164 GRO73461 486 0.0156265071863\n",
      "39 DAI62779 FRO78087 482 0.0154978939584\n",
      "40 DAI85309 ELE17451 482 0.0154978939584\n",
      "41 DAI62779 GRO94758 479 0.0154014340375\n",
      "42 DAI62779 GRO21487 471 0.0151442075817\n",
      "43 GRO85051 SNA80324 471 0.0151442075817\n",
      "44 ELE17451 GRO30386 468 0.0150477476608\n",
      "45 FRO85978 SNA95666 463 0.014886981126\n",
      "46 DAI62779 FRO19221 462 0.014854827819\n",
      "47 DAI62779 GRO46854 461 0.0148226745121\n",
      "48 DAI43223 DAI62779 459 0.0147583678981\n",
      "49 ELE92920 SNA18336 455 0.0146297546703\n",
      "50 DAI88079 FRO40251 446 0.0143403749076\n"
     ]
    }
   ],
   "source": [
    "from StripesTop50HW252 import StripesTop50\n",
    "mr_job = StripesTop50(args=['ProductPurchaseData.txt']) \n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    counter = 1      \n",
    "    for line in runner.stream_output():\n",
    "        pair, frequencies =  mr_job.parse_output_line(line)\n",
    "        print counter, pair[0].upper(), pair[1].upper(), frequencies[0], frequencies[1]\n",
    "        counter += 1\n",
    "        if counter > 50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing StripesHW251_runnertimer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile StripesHW251_runnertimer.py\n",
    "from StripesHW251 import Stripes\n",
    "mr_job = Stripes(args=['ProductPurchaseData.txt', '--jobconf=\"mapred.map.tasks=2\"', '--jobconf=\"mapred.reduce.tasks=2\"']) \n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75 ms, sys: 27.6 ms, total: 103 ms\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%time !python StripesHW251_runnertimer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine used was a quad core Mac, running on OS X.  Mapper/Reducer count set to 2.  Number of Mapper, Reducer, Combiner, MapperSort and ReducerSort calls in the results above.  The Stripes Method ran substantially faster than the Pairs method.  Also, the number of combiners and reducers was dramatically reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'FunctionCalls': {'Mapper': 31101, 'Reducer': 877095, 'Combiner': 1026707}}]\r\n"
     ]
    }
   ],
   "source": [
    "!python PairsHW241_runnertimer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python StripesHW251_runnertimer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>Pairs Stats</th>\n",
       "      <th>Stripes Stats</th>\n",
       "      <th>Reduction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mapper Calls</td>\n",
       "      <td>31101</td>\n",
       "      <td>31101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reducer Calls</td>\n",
       "      <td>877095</td>\n",
       "      <td>12593</td>\n",
       "      <td>864502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Combiner Calls</td>\n",
       "      <td>1026707</td>\n",
       "      <td>17747</td>\n",
       "      <td>1008960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Metrics Pairs Stats Stripes Stats  Reduction\n",
       "0    Mapper Calls       31101         31101          0\n",
       "1   Reducer Calls      877095         12593     864502\n",
       "2  Combiner Calls     1026707         17747    1008960"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "from IPython.display import display\n",
    "pairsdata = {'Mapper Calls': '31101', 'Combiner Calls': '1026707', 'Reducer Calls': '877095'}\n",
    "pairsDF = pandas.DataFrame({\"Metrics\": pairsdata.keys(), \"Pairs Stats\": pairsdata.values()})\n",
    "\n",
    "stripesdata = {'Mapper Calls': '31101', 'Combiner Calls': '17747', 'Reducer Calls': '12593'}\n",
    "stripesDF = pandas.DataFrame({\"Metrics\": stripesdata.keys(), \"Stripes Stats\": stripesdata.values()})\n",
    "pair_stripe_comp = pandas.merge(pairsDF, stripesDF, how='inner')\n",
    "\n",
    "pair_stripe_comp = pandas.merge(pairsDF, stripesDF, how='inner')\n",
    "reduction = {'Mapper Calls': 31101 - 31101, 'Combiner Calls': 1026707 - 17747, 'Reducer Calls': 877095 - 12593}\n",
    "reductionDF = pandas.DataFrame({\"Metrics\": reduction.keys(), \"Reduction\": reduction.values()})\n",
    "comparison = pandas.merge(pair_stripe_comp, reductionDF, how='inner')\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of Comparison between Pairs and Stripes:\n",
    "* Expectedly, the Mapper call was not changed, as the mapper is called for each line in the file.  \n",
    "* The combiner is called substantially less in the Stripes method because the Pairs method creates significantly more key-value pairs.  \n",
    "* The Reducer is called substantially less in the Stripes method for the same reason the combiners are reduced (fewer key-value pairs created in Stripes than in Pairs method)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
