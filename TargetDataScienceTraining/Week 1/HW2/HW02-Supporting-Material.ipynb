{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 Supporting Material\n",
    "James G. Shanahan 6/28/201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a name=\"TOC\"></a> \n",
    "  \n",
    "1.  [Introduction](#1)   \n",
    "2.  [Counters in MRJob](#2)   \n",
    "3.  [JSON and MRJob Serialization](#3)   \n",
    "4.  [Calculate Summary Stats](#4) \n",
    "    1.  [calculate salary average](#4.1)\n",
    "    2.  [calculate mean and variance](#4.2)\n",
    "5.  [Order inversion pattern in MRJob](#5)\n",
    "0.  [Useful Links for Jupyter Notebooks](#0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Introduction <a name=\"1\"></a> \n",
    "[Back to Table of Contents](#TOC)\n",
    "    \n",
    "You can view this notebook via NBViewer [here](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/pjd6maluq4ogt7m/HW02-Supporting-Material.ipynb#4.1). \n",
    "+ http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/pjd6maluq4ogt7m/HW02-Supporting-Material.ipynb#4.1\n",
    "\n",
    "This notebook provides several examples of key patterns that you will use repeatedly in HW2. Please run and extend these examples to get warmed up before tackling HW2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Counters in MRJob <a name=\"2\"></a> \n",
    "[Back to Table of Contents](#TOC)\n",
    "    \n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. Think of them as global variables with read/write constraints\n",
    "\n",
    "Here is an example of them in use:\n",
    "\n",
    "   + http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/5thl14n4pqvhzt5/Counter.ipynb \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: wget: command not found\r\n"
     ]
    }
   ],
   "source": [
    "#download the book great-expectations.txt\n",
    "! mkdir output-document=data\n",
    "!wget --output-document=data/great-expectations.txt http://www.gutenberg.org/cache/epub/1400/pg1400.txt\n",
    "    \n",
    "# OR use\n",
    "#!curl http://www.gutenberg.org/cache/epub/1400/pg1400.txt > great-expectations.txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "!head great-expectations.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  JSON and MRJob Serialization <a name=\"3\"></a> \n",
    "[Back to Table of Contents](#TOC)\n",
    "    \n",
    "<LI> examples of work with JSON data (key-value data structures; like dictionaries)\n",
    "\n",
    "<LI> MRJob Serialization  + JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: JSON: File exists\r\n"
     ]
    }
   ],
   "source": [
    "mkdir JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON/data.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile JSON/data.json\n",
    "{\n",
    " \"maps\":[\n",
    "         {\"id\":\"blabla\",\"iscategorical\":\"0\"},\n",
    "         {\"id\":\"blabla\",\"iscategorical\":\"0\"}\n",
    "        ],\n",
    "\"masks\":\n",
    "         {\"id\":\"valore\"},\n",
    "\"om_points\":\"value\",\n",
    "\"parameters\":\n",
    "         {\"id\":\"valore\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'maps': [{u'id': u'blabla', u'iscategorical': u'0'},\n",
      "           {u'id': u'blabla', u'iscategorical': u'0'}],\n",
      " u'masks': {u'id': u'valore'},\n",
      " u'om_points': u'value',\n",
      " u'parameters': {u'id': u'valore'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open('JSONtest/data.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blabla\n",
      "valore\n",
      "value\n"
     ]
    }
   ],
   "source": [
    "print data[\"maps\"][0][\"id\"]  # will return 'blabla'\n",
    "print data[\"masks\"][\"id\"]    # will return 'valore'\n",
    "print data[\"om_points\"]      # will return 'value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON/chineseExample.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile JSON/chineseExample.txt\n",
    "D1\t1\tChinese Beijing\tChinese\n",
    "D2\t1\tChinese Chinese\tShanghai\n",
    "D3\t1\tChinese\tMacao\n",
    "D4\t0\tTokyo Japan\tChinese\n",
    "D5\t0\tChinese Chinese\tChinese Tokyo Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON/chineseExampleJSON.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile JSON/chineseExampleJSON.txt\n",
    "{\"email\": {\"id\": \"D1\", \"Label\": \"1\", \"content\": {\"subject\":\"Chinese Beijing\", \"body\":\"Chinese\" } } }\n",
    "{\"email\": {\"id\": \"D2\", \"Label\": \"1\", \"content\": {\"subject\":\"Chinese Chinese\", \"body\":\"Shanghai\" } } }\n",
    "{\"email\": {\"id\": \"D3\", \"Label\": \"1\", \"content\": {\"subject\":\"Chinese\", \"body\":\"Macao\" } } }\n",
    "{\"email\": {\"id\": \"D4\", \"Label\": \"0\", \"content\": {\"subject\":\"Tokyo Japan\", \"body\":\"Chinese\" } } }\n",
    "{\"email\": {\"id\": \"D5\", \"Label\": \"0\", \"content\": {\"subject\":\"Chinese Chinese\", \"body\":\"Chinese Tokyo Japan\" } } }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading-and-parsing-a-json-file-in-python\n",
    "For more background see [loading-and-parsing-a-json-file-in-python](http://stackoverflow.com/questions/12451431/loading-and-parsing-a-json-file-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mkdir JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the entire file's content [ Jimi wrote hello in the file\n",
      "line2 of this dribble ]\n",
      "\n",
      "line Jimi wrote hello in the file\n",
      "\n",
      "line line2 of this dribble\n",
      "read a line at time Jimi wrote hello in the file\n",
      "\n",
      "read a line at time line2 of this dribble\n"
     ]
    }
   ],
   "source": [
    "# Open a file\n",
    "filename = \"JSON/foo.txt\"\n",
    "fo = open(filename, \"w\")\n",
    "fo.write(\"Jimi wrote hello in the file\\nline2 of this dribble\");\n",
    "# Close opend file\n",
    "fo.close()\n",
    "\n",
    "\n",
    "# Open a file and read the file contents\n",
    "fo = open(filename, \"r\")\n",
    "str = fo.read();\n",
    "print \"This is the entire file's content [\", str, \"]\\n\"\n",
    "# Close opend file\n",
    "fo.close()\n",
    "\n",
    "# read all lines together into a list \n",
    "with open(filename) as f:\n",
    "    content = f.readlines()\n",
    "for l in content:\n",
    "    print \"line\", l\n",
    "\n",
    "#read the file line by line\n",
    "with open(filename, \"r\") as ins:\n",
    "    array = []\n",
    "    for line in ins:\n",
    "        array.append(line)\n",
    "for l in content:\n",
    "    print \"read a line at time\", l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Write to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'email': {u'Label': u'1',\n",
      "            u'content': {u'body': u'Chinese', u'subject': u'Chinese Beijing'},\n",
      "            u'id': u'D1'}}\n",
      "{u'email': {u'Label': u'1',\n",
      "            u'content': {u'body': u'Shanghai',\n",
      "                         u'subject': u'Chinese Chinese'},\n",
      "            u'id': u'D2'}}\n",
      "{u'email': {u'Label': u'1',\n",
      "            u'content': {u'body': u'Macao', u'subject': u'Chinese'},\n",
      "            u'id': u'D3'}}\n",
      "{u'email': {u'Label': u'0',\n",
      "            u'content': {u'body': u'Chinese', u'subject': u'Tokyo Japan'},\n",
      "            u'id': u'D4'}}\n",
      "{u'email': {u'Label': u'0',\n",
      "            u'content': {u'body': u'Chinese Tokyo Japan',\n",
      "                         u'subject': u'Chinese Chinese'},\n",
      "            u'id': u'D5'}}\n",
      "[{u'email': {u'content': {u'body': u'Chinese', u'subject': u'Chinese Beijing'}, u'id': u'D1', u'Label': u'1'}}, {u'email': {u'content': {u'body': u'Shanghai', u'subject': u'Chinese Chinese'}, u'id': u'D2', u'Label': u'1'}}, {u'email': {u'content': {u'body': u'Macao', u'subject': u'Chinese'}, u'id': u'D3', u'Label': u'1'}}, {u'email': {u'content': {u'body': u'Chinese', u'subject': u'Tokyo Japan'}, u'id': u'D4', u'Label': u'0'}}, {u'email': {u'content': {u'body': u'Chinese Tokyo Japan', u'subject': u'Chinese Chinese'}, u'id': u'D5', u'Label': u'0'}}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "#assume one JSON dictionary per line (otherwise one will have to write a custom parser; a little more effort)\n",
    "data = []\n",
    "with open('JSON/chineseExampleJSON.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "for d in data:\n",
    "    pprint(d)\n",
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'email': {u'content': {u'body': u'Chinese', u'subject': u'Chinese Beijing'}, u'id': u'D1', u'Label': u'1'}}, {u'email': {u'content': {u'body': u'Shanghai', u'subject': u'Chinese Chinese'}, u'id': u'D2', u'Label': u'1'}}, {u'email': {u'content': {u'body': u'Macao', u'subject': u'Chinese'}, u'id': u'D3', u'Label': u'1'}}, {u'email': {u'content': {u'body': u'Chinese', u'subject': u'Tokyo Japan'}, u'id': u'D4', u'Label': u'0'}}, {u'email': {u'content': {u'body': u'Chinese Tokyo Japan', u'subject': u'Chinese Chinese'}, u'id': u'D5', u'Label': u'0'}}]\n"
     ]
    }
   ],
   "source": [
    "#print a string of records\n",
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entire record as a string  {u'email': {u'content': {u'body': u'Chinese', u'subject': u'Chinese Beijing'}, u'id': u'D1', u'Label': u'1'}}\n",
      "attributes of the email record as string --> {u'content': {u'body': u'Chinese', u'subject': u'Chinese Beijing'}, u'id': u'D1', u'Label': u'1'}\n",
      "label of email --> 1\n",
      "body of email --> Chinese\n",
      "body of email from CHOPPED record --> Chinese\n"
     ]
    }
   ],
   "source": [
    "#print first record and various attributes\n",
    "print \"entire record as a string \", data[0]  \n",
    "print \"attributes of the email record as string -->\", data[0][\"email\"] \n",
    "print \"label of email -->\", data[0][\"email\"][\"Label\"] \n",
    "print \"body of email -->\", data[0][\"email\"][\"content\"][\"body\"] \n",
    "\n",
    "# chopped off part of the JSON dictionary\n",
    "# it is STILL a JSON dictionary (just a smaller subset of its parent) \n",
    "x=data[0][\"email\"]\n",
    "print \"body of email from CHOPPED record -->\", x[\"content\"][\"body\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mrjob.protocol - input and output\n",
    "\n",
    "Protocols translate raw bytes into key, value pairs.\n",
    "\n",
    "Typically, protocols encode a key and value into bytes, and join them together with a tab character.\n",
    "\n",
    "However, protocols with Value in their name ignore keys and simply read/write values (with key read in as None), allowing you to read and write data in arbitrary formats.\n",
    "\n",
    "For more information, see [Protocols and Writing custom protocols](https://pythonhosted.org/mrjob/protocols.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# JSON Protocol for serialization\n",
    "\n",
    "MRJob provides a seamless JSON reader and writer (i.e the mapper can read json lines and convert them into   lists)\n",
    "We can test hadoop job locally (in windows or unix) on a small dataset without actually using huge hdfs files (quick !!)\n",
    "Can orchestrate many mappers and reducers in the same code \n",
    "\n",
    "My task is to parse json formatted web log files and parse them, say the columns are sessionid,stepno and data. so the psuedo-code\n",
    "Read the json files using mrjob protocol\n",
    "        DEFAULT_INPUT_PROTOCOL = 'json_value'\n",
    "        DEFAULT_OUTPUT_PROTOCOL = 'repr_value'  #  output is delimited\n",
    "\n",
    "  2.  yield sessionid, (sessionid,stepno,data) from mapper\n",
    "                      the Mapreduce will make sure that all sessionids(key) goes to same mapper.. with the remaining values sent as a dictionary (value) to make it easier for us to srt in reducer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ProcessJSONRecords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ProcessJSONRecords.py\n",
    "import sys,time\n",
    "#sys.path.append('/usr/lib/python2.4/site-packages/')\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class ProcessJSONRecords(MRJob):\n",
    "    DEFAULT_INPUT_PROTOCOL = 'json_value'\n",
    "    DEFAULT_OUTPUT_PROTOCOL = 'repr_value'\n",
    "\n",
    "    def mapper(self, _, lineStr):\n",
    "        line = json.loads(lineStr)\n",
    "        print \"full JSON Dictionary\", line\n",
    "        emailID = line['email']['id']\n",
    "        print \"emailID is \", emailID\n",
    "        label = line['email']['Label']\n",
    "        content = line[\"email\"][\"content\"]\n",
    "        print line[\"email\"][\"Label\"], line[\"email\"][\"content\"]\n",
    "        yield line[\"email\"][\"Label\"], line[\"email\"][\"content\"]\n",
    "\n",
    "    def reducer(self, label, emailBodies):\n",
    "        #for bodyText in emailBodies:\n",
    "            #line_data='\\t'.join(str(n) for n in d)\n",
    "        #    yield label, str(bodyText)\n",
    "        print label+\"Reducer\", emailBodies\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ProcessJSONRecords.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"email\": {\"id\": \"D1\", \"Label\": \"1\", \"content\": {\"subject\":\"Chinese Beijing\", \"body\":\"Chinese\" } } }\r\n",
      "{\"email\": {\"id\": \"D2\", \"Label\": \"1\", \"content\": {\"subject\":\"Chinese Chinese\", \"body\":\"Shanghai\" } } }\r\n",
      "{\"email\": {\"id\": \"D3\", \"Label\": \"1\", \"content\": {\"subject\":\"Chinese\", \"body\":\"Macao\" } } }\r\n",
      "{\"email\": {\"id\": \"D4\", \"Label\": \"0\", \"content\": {\"subject\":\"Tokyo Japan\", \"body\":\"Chinese\" } } }\r\n",
      "{\"email\": {\"id\": \"D5\", \"Label\": \"0\", \"content\": {\"subject\":\"Chinese Chinese\", \"body\":\"Chinese Tokyo Japan\" } } }"
     ]
    }
   ],
   "source": [
    "!head JSON/chineseExampleJSON.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\r\n",
      "no configs found; falling back on auto-configuration\r\n",
      "creating tmp directory /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/ProcessJSONRecords.jshanahan.20160629.060733.041354\r\n",
      "writing to /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/ProcessJSONRecords.jshanahan.20160629.060733.041354/step-0-mapper_part-00000\r\n",
      "full JSON Dictionary {u'email': {u'content': {u'body': u'Chinese', u'subject': u'Chinese Beijing'}, u'id': u'D1', u'Label': u'1'}}\r\n",
      "emailID is  D1\r\n",
      "1 {u'body': u'Chinese', u'subject': u'Chinese Beijing'}\r\n",
      "full JSON Dictionary {u'email': {u'content': {u'body': u'Shanghai', u'subject': u'Chinese Chinese'}, u'id': u'D2', u'Label': u'1'}}\r\n",
      "emailID is  D2\r\n",
      "1 {u'body': u'Shanghai', u'subject': u'Chinese Chinese'}\r\n",
      "full JSON Dictionary {u'email': {u'content': {u'body': u'Macao', u'subject': u'Chinese'}, u'id': u'D3', u'Label': u'1'}}\r\n",
      "emailID is  D3\r\n",
      "1 {u'body': u'Macao', u'subject': u'Chinese'}\r\n",
      "full JSON Dictionary {u'email': {u'content': {u'body': u'Chinese', u'subject': u'Tokyo Japan'}, u'id': u'D4', u'Label': u'0'}}\r\n",
      "emailID is  D4\r\n",
      "0 {u'body': u'Chinese', u'subject': u'Tokyo Japan'}\r\n",
      "full JSON Dictionary {u'email': {u'content': {u'body': u'Chinese Tokyo Japan', u'subject': u'Chinese Chinese'}, u'id': u'D5', u'Label': u'0'}}\r\n",
      "emailID is  D5\r\n",
      "0 {u'body': u'Chinese Tokyo Japan', u'subject': u'Chinese Chinese'}\r\n",
      "Counters from step 1:\r\n",
      "  (no counters found)\r\n",
      "writing to /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/ProcessJSONRecords.jshanahan.20160629.060733.041354/step-0-mapper-sorted\r\n",
      "> sort /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/ProcessJSONRecords.jshanahan.20160629.060733.041354/step-0-mapper_part-00000\r\n",
      "writing to /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/ProcessJSONRecords.jshanahan.20160629.060733.041354/step-0-reducer_part-00000\r\n",
      "0Reducer <generator object <genexpr> at 0x1033be190>\r\n",
      "1Reducer <generator object <genexpr> at 0x1033be1e0>\r\n",
      "Counters from step 1:\r\n",
      "  (no counters found)\r\n",
      "Moving /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/ProcessJSONRecords.jshanahan.20160629.060733.041354/step-0-reducer_part-00000 -> /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/ProcessJSONRecords.jshanahan.20160629.060733.041354/output/part-00000\r\n",
      "Streaming final output from /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/ProcessJSONRecords.jshanahan.20160629.060733.041354/output\r\n",
      "removing tmp directory /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/ProcessJSONRecords.jshanahan.20160629.060733.041354\r\n"
     ]
    }
   ],
   "source": [
    "!python ProcessJSONRecords.py --jobconf mapred.reduce.tasks=1 JSON/chineseExampleJSON.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NaiveBayes/chineseExampleJSON.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Calculate Summary Stats <a name=\"4\"></a> \n",
    "[Back to Table of Contents](#TOC)\n",
    "    \n",
    "\n",
    "<LI> Calculate Average Salary\n",
    "<LI> Calculate Mean and Variance/Standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1  Calculate Average Salary <a name=\"4.1\"></a> \n",
    "[Back to Table of Contents](#TOC)\n",
    "    1.  [calculate salary average](#4.1)\n",
    "\n",
    "For more background on this example see:  Chapter 2 in this book: \"Hadoop with MRJob\"  \n",
    "\n",
    "<LI> https://www.dropbox.com/s/jd3z2s216p9kc1z/hadoop-with-python-MRJOB.pdf?dl=0\n",
    "<LI> Source code: https://www.dropbox.com/sh/j8oettuxbgztk0p/AAAwq9PpEeByecDmaSNslnBPa?dl=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  ,06/10/2013,$11310.00,$873.63\n",
      "\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev ,10/24/1979,$53428.00,$52868.38\n",
      "\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office ,09/25/2006,$68300.00,$67439.19\n",
      "\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department ,07/23/2009,$62000.00,$58654.74\n",
      "\"Abbene,Anthony M\",POLICE OFFICER TRAINEE,A99416,Police Department ,07/24/2013,$43999.00,$39686.95\n",
      "\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology ,05/01/2013,$52000.00,$47019.75\n",
      "\"Abdal-Rahim,Naim A\",EMT Firefighter Suppression,A64120,Fire Department ,03/30/2011,$62175.00,$61451.50\n",
      "\"Abdi,Ezekiel W\",POLICE SERGEANT,A99127,Police Department ,06/14/2007,$70918.00,$87900.27\n",
      "\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office ,09/02/1999,$42438.00,$53667.53\n",
      "\"Abdul Aziz,Hajr E\",AIDE BLUE CHIP,W02097,Youth Summer  ,06/18/2014,$11310.00,\n",
      "-------------------\n",
      "\"Zimmerman,Jeremy E\",POLICE OFFICER,A99123,Police Department ,04/15/2013,$44104.00,$47495.25\n",
      "\"Zimmerman,Karl J\",FIRE COMMAND STAFF II,A64006,Fire Department ,03/26/1990,$129587.00,$131703.92\n",
      "\"Zoppo Jr,Phillip A\",MASON SUPERVISOR,A50509,DPW-Water & Waste Water ,04/04/1988,$52171.00,$61386.26\n",
      "\"Zoppo,Catherine B\",REAL ESTATE AGENT I,A15001,COMP-Real Estate ,03/14/1977,$58313.00,$58359.66\n",
      "\"Zorbach,Michael K\",POLICE OFFICER,A99224,Police Department ,05/02/2012,$44104.00,$48915.33\n",
      "\"Zotamou,Jean Marie D\",AIDE BLUE CHIP,W02235,Youth Summer  ,05/21/2014,$11310.00,\n",
      "\"Zotamou,Pivot D\",AIDE BLUE CHIP,W02629,Youth Summer ,05/21/2014,$11310.00,\n",
      "\"Zovistoski,Zachary D\",POLICE OFFICER TRAINEE,A99416,Police Department ,12/17/2013,$43999.00,$21070.03\n",
      "\"Zubyk,Stanislav T\",POLICE OFFICER,A99262,Police Department ,01/23/2013,$44104.00,$48608.12\n",
      "\"Zukowski,Charles J\",Waste Water Tech Supv I Pump,A50206,DPW-Water & Waste Water ,10/15/1979,$53568.00,$52164.32"
     ]
    }
   ],
   "source": [
    "# salaries file is located here :\n",
    "#   https://www.dropbox.com/s/sp87kq56achu8po/salaries.csv?dl=0\n",
    "!head salaries.csv\n",
    "!echo \"-------------------\"\n",
    "!tail salaries.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "\n",
    "cols = 'Name,JobTitle,AgencyID,Agency,HireDate,AnnualSalary,GrossPay'.split(',')\n",
    "\n",
    "\n",
    "class salaryavg(MRJob):\n",
    "\n",
    "    def avgmapper(self, _, line):\n",
    "        row = dict(zip(cols, [ a.strip() for a in csv.reader([line]).next()]))\n",
    "\n",
    "        self.increment_counter('depts', row['Agency'], 1)\n",
    "\n",
    "        yield row['JobTitle'], (int(float(row['AnnualSalary'][1:])), 1)\n",
    "\n",
    "    def avgreducer(self, key, values):\n",
    "        s = 0\n",
    "        c = 0\n",
    "\n",
    "        for average, count in values:\n",
    "            s += average * count\n",
    "            c += count\n",
    "\n",
    "        if c > 3:\n",
    "            self.increment_counter('stats', 'below3', 1)\n",
    "            yield key, (s/c, c)\n",
    "\n",
    "    def ttmapper(self, key, value):\n",
    "        yield None, (value[0], key) # group by all, keep average and job title\n",
    "\n",
    "    def ttreducer(self, key, values):\n",
    "        topten = []\n",
    "        for average, job in values:\n",
    "            topten.append((average, job))\n",
    "            topten.sort()\n",
    "            topten = topten[-10:]\n",
    "\n",
    "        for average, job in topten:\n",
    "            yield None, (average, job)\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.avgmapper,\n",
    "                   combiner=self.avgreducer,\n",
    "                   reducer=self.avgreducer),\n",
    "            MRStep(mapper=self.ttmapper,\n",
    "                   combiner=self.ttreducer,\n",
    "                   reducer=self.ttreducer) ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    salaryavg.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "\n",
    "cols = 'Name,JobTitle,AgencyID,Agency,HireDate,AnnualSalary,GrossPay'.split(',')\n",
    "\n",
    "class salarymax(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Convert each line into a dictionary\n",
    "        row = dict(zip(cols, [ a.strip() for a in csv.reader([line]).next()]))\n",
    "\n",
    "        # Yield the salary\n",
    "        yield 'salary', (float(row['AnnualSalary'][1:]), line)\n",
    "        \n",
    "        # Yield the gross pay\n",
    "        try:\n",
    "            yield 'gross', (float(row['GrossPay'][1:]), line)\n",
    "        except ValueError:\n",
    "            self.increment_counter('warn', 'missing gross', 1)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        topten = []\n",
    "\n",
    "        # For 'salary' and 'gross' compute the top 10\n",
    "        for p in values:\n",
    "            topten.append(p)\n",
    "            topten.sort()\n",
    "            topten = topten[-10:]\n",
    "\n",
    "        for p in topten:\n",
    "            yield key, p\n",
    "\n",
    "    combiner = reducer\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    salarymax.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "\n",
    "   def mapper(self, _, line):\n",
    "      for word in line.split():\n",
    "         yield(word, 1)\n",
    "\n",
    "   def reducer(self, word, counts):\n",
    "      yield(word, sum(counts))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   MRWordCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2  Calculate Summary Stats <a name=\"4.2\"></a> \n",
    "[Back to Table of Contents](#TOC)\n",
    "    \n",
    "\n",
    "<LI> Calculate Average Salary\n",
    "<LI> Calculate Mean and Variance/Standard deviation\n",
    "\n",
    "\n",
    "See  http://www.statisticslectures.com/topics/variancesample/\n",
    "<p>\n",
    "[Variance](https://www.dropbox.com/s/0k8zal1nncdygji/Screenshot%202016-06-26%2004.17.15.png?dl=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir Combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Variance_STDEV.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"Variance_STDEV.png\")\n",
    "\n",
    "# image can be downloaded from here:  \n",
    "#       https://www.dropbox.com/s/al3uu3tjqs17ozz/Variance_STDEV.png?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Combiner/example.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile Combiner/example.txt\n",
    "{\"id\":1, \"kind\":\"Truc\",   \"value\": 4}\n",
    "{\"id\":2, \"kind\":\"Machin\", \"value\": 5}\n",
    "{\"id\":3, \"kind\":\"Machin\", \"value\": 15}\n",
    "{\"id\":4, \"kind\":\"Chose\",  \"value\": 3}\n",
    "{\"id\":5, \"kind\":\"Truc\",   \"value\": 20}\n",
    "{\"id\":6, \"kind\":\"Chose\",  \"value\": 3}\n",
    "{\"id\":7, \"kind\":\"Truc\",   \"value\": 6}\n",
    "{\"id\":8, \"kind\":\"Truc\",   \"value\": 4000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Combiner/mrMeanVar.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Combiner/mrMeanVar.py\n",
    "from mrjob.job import MRJob\n",
    "from math import sqrt   \n",
    "import json\n",
    "\n",
    "# given 4,5,15,3,20, 3,6,4000\n",
    "# Mean = 507, SD = 1411\n",
    "# Calculate the mean and standard deviation\n",
    "#\n",
    "class mrMeanVar(MRJob):\n",
    "    DEFAULT_PROTOCOL = 'json'  #split records in key value pairs using TAB\n",
    "\n",
    "    def mapper(self, key, line):\n",
    "        lineDict = json.loads(line)\n",
    "        val = lineDict[\"value\"]\n",
    "        yield 1,(val, val*val)\n",
    "        \n",
    "    \n",
    "\n",
    "    def reducer(self, key, vals):\n",
    "        N = 0.0\n",
    "        sum = 0.0\n",
    "        sumsq = 0.0\n",
    "        for val, valSqd in vals:\n",
    "            N += 1\n",
    "            sum += val\n",
    "            sumsq += valSqd\n",
    "        mean = sum/N\n",
    "        sd = sqrt((sumsq - sum*sum/N)/(N-1))\n",
    "        results = [mean,sd]\n",
    "        yield 1,results\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    mrMeanVar.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/mrMeanVar.jshanahan.20160626.113155.399710\n",
      "writing to /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/mrMeanVar.jshanahan.20160626.113155.399710/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/mrMeanVar.jshanahan.20160626.113155.399710/step-0-mapper-sorted\n",
      "> sort /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/mrMeanVar.jshanahan.20160626.113155.399710/step-0-mapper_part-00000\n",
      "writing to /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/mrMeanVar.jshanahan.20160626.113155.399710/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/mrMeanVar.jshanahan.20160626.113155.399710/step-0-reducer_part-00000 -> /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/mrMeanVar.jshanahan.20160626.113155.399710/output/part-00000\n",
      "Streaming final output from /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/mrMeanVar.jshanahan.20160626.113155.399710/output\n",
      "1\t[507.0, 1411.3989007870373]\n",
      "removing tmp directory /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/mrMeanVar.jshanahan.20160626.113155.399710\n"
     ]
    }
   ],
   "source": [
    "!python Combiner/mrMeanVar.py --jobconf mapred.reduce.tasks=1 Combiner/example.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Order inversion pattern in MRJob <a name=\"5\"></a> \n",
    "[Back to Table of Contents](#TOC)\n",
    "    \n",
    "\n",
    "For more background on this example see:  Chapter 2 in this book: \"Hadoop with MRJob\"  \n",
    "\n",
    "<LI> https://www.dropbox.com/s/jd3z2s216p9kc1z/hadoop-with-python-MRJOB.pdf?dl=0\n",
    "<LI> Source code: https://www.dropbox.com/sh/j8oettuxbgztk0p/AAAwq9PpEeByecDmaSNslnBPa?dl=0\n",
    "### Order inversion pattern in MRJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCENARIO\n",
    "I am trying to yield the probability each key,value pair generated from mapper has.\n",
    "\n",
    "So, lets say mapper yields:\n",
    "\n",
    "a, (r, 5)\n",
    "a, (e, 6)\n",
    "a, (w, 7)\n",
    "I need to add 5+6+7 = 18 and then find probabilities 5/18, 6/18, 7/18\n",
    "\n",
    "so the final output from the reducer would look like:\n",
    "\n",
    "a, [[r, 5, 0.278], [e, 6, 0.33], [w, 7, 0.389]]\n",
    "so far, I can only get the reducer to sum all integers from the value. How can I make it to go back and divide each instance by the total sum?\n",
    "\n",
    "##### References\n",
    "<LI>http://stackoverflow.com/questions/15051137/mrjob-can-a-reducer-perform-2-operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you are doing above should work as well, but this is assuming that all of the data for a single key will fit in memory. If it does, then at Reducer you can hold all values in memory and then compute your total to then calculate the marginal for each key-value pair. This is commonly known as the \"stripes\" approach.\n",
    "\n",
    "However, most of the times this might now be true and the data might not fit in memory. In this case you will have to find a way to send values to compute your total before the actual key-value pair so that when they can then be used to compute the marginal and emit the value right away.\n",
    "\n",
    "This is a candidate for the \"order of inversion\" design pattern. Its useful when you need to calculate relative frequencies. For Hadoop, The basic idea is at the Mapper's end you emit 2 key-value pairs for each intermediate data where one of the key-value pair  but will have the same common key for all values. This will be used to calculate the total. For MRJob, this is greatly simplified by using the \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Example\n",
    "For a, (r, 5) :\n",
    "---------------\n",
    "emit (a), r, 5\n",
    "emit (a), *, 5\n",
    "\n",
    "\n",
    "For a, (e, 6) :\n",
    "---------------\n",
    "emit (a), e, 6\n",
    "emit (a), *, 6\n",
    "\n",
    "\n",
    "For a, (w, 7) :\n",
    "---------------\n",
    "emit (a), w, 7\n",
    "emit (a), *, 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, you need a partitioner that will partition each of the intermediate key-value pair using only the first value in the key. In the example above using \"a\". \n",
    "\n",
    "You will also need a key sort order that always places the key having * in the second part of the key above all.\n",
    "\n",
    "This way all intermediate keys have \"a\" in the first part of the key will end up in the same reducer. Also, they will sorted in a fashion as shown below -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emit (a, *), 5\n",
    "emit (a, *), 6\n",
    "emit (a, *), 7\n",
    "emit (a, e), 6\n",
    "emit (a, r), 5\n",
    "emit (a, w), 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the reducer as you iterate through the key-value pairs, you will have to simply accumulate the values from the keys if they have a * in the second part of the key. You can then use the accumulated value to calculate your marginal for all the other key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = 0\n",
    "for(value : values){\n",
    "    if (key.second == *)\n",
    "        total += value\n",
    "    else\n",
    "        emit (key.first , key.second, value, value/total)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This design pattern is commonly known as Order of inversion that uses the pairs approach. For more info on this and other design patterns I would suggest reading the chapter on MapReduce design patterns in this book - http://lintool.github.com/MapReduceAlgorithms/. It very well explained with examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrRelativeProbs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrRelativeProbs.py\n",
    "\n",
    "\"\"\"\n",
    "Calculate the Pr(term |class)\n",
    "use the order inversion pattern (which uses secondary sort)\n",
    "\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MrRelativeProbs(MRJob):\n",
    "    DEFAULT_PROTOCOL = 'json'  #split records in key value pairs using TAB\n",
    "    \n",
    "    # Performs secondary sort on the word with in the class\n",
    "    # as a result the Reducer receives records sort by class, and then by word (\n",
    "    # remember we have a special word **Total\n",
    "    SORT_VALUES = True\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MrRelativeProbs, self).__init__(*args, **kwargs)\n",
    "        self.modelStats = {}\n",
    "\n",
    " \n",
    "    def mapper(self, key, line):\n",
    "        docID, docClass, text = line.split(\"\\t\",2)   \n",
    "        words = text.split()\n",
    "        for w in words:\n",
    "            yield docClass, (w, 1)\n",
    "        yield docClass, (\"**Total\", len(words)) # keep a tally of the total\n",
    "        \n",
    "    # TODO: Should add a combiner\n",
    "    \n",
    "    def reducer(self, classI, wordFreqs):\n",
    "        #print \"wordFreqs\", wordFreqs\n",
    "        #for w, freq in wordFreqs:   NOTE that this for loop exhausted the iterator; so can not iterate again later\n",
    "        #      print \"wordFreqs\", classI, w, freq\n",
    "        total = 0   #start of a new class\n",
    "        currentWord=\"\"\n",
    "        currentWordFreq=0\n",
    "        for w, freq  in wordFreqs:\n",
    "            if (w == \"**Total\") : \n",
    "                total += freq\n",
    "            elif currentWord == w:\n",
    "                currentWordFreq += freq\n",
    "            else: # a new word\n",
    "                if currentWord != \"**Total\" and currentWord != \"\":\n",
    "                    yield(classI, [currentWord, currentWordFreq, float(currentWordFreq)/total])\n",
    "                currentWord=w\n",
    "                currentWordFreq=freq\n",
    "        #dont forget the last word!\n",
    "        if currentWord !=\"\" and currentWord != \"**Total\" :\n",
    "            yield(classI, (currentWord, currentWordFreq, float(currentWordFreq)/total))\n",
    " \n",
    "\n",
    "# The if __name__ == \"__main__\": \n",
    "# ... trick exists in Python so that our Python files \n",
    "# can act as either reusable modules, or as standalone programs.\n",
    "if __name__ == '__main__':\n",
    "    MrRelativeProbs.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/jshanahan/Dropbox/Projects/Target-2016-04/Homeworks/HW02'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join two tables (using a secondary sort)\n",
    "\n",
    "The following Performs secondary sort <p>\n",
    "  SORT_VALUES = True\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Join/customers.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile Join/customers.dat\n",
    "Alice Bob|not bad|US\n",
    "Sam Sneed|valued|CA\n",
    "Jon Sneed|valued|CA\n",
    "Arnold Wesise|not so good|UK\n",
    "Henry Bob|not bad|US\n",
    "Yo Yo Ma|not so good|CA\n",
    "Jon York|valued|CA\n",
    "Alex Ball|valued|UK\n",
    "Jim Davis|not so bad|JA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Join/countries.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile Join/countries.dat\n",
    "United States|US\n",
    "Canada|CA\n",
    "United Kingdom|UK\n",
    "Italy|IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Join/MRJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Join/MRJoin.py\n",
    "# Adapted for MrJob from Joe Stein's example at:\n",
    "# http://allthingshadoop.com/2011/12/16/simple-hadoop-streaming-tutorial-using-joins-and-keys-with-python/\n",
    "\n",
    "import sys, os, re\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "  \n",
    "  # Performs secondary sort\n",
    "  SORT_VALUES = True\n",
    "  \n",
    "  def mapper(self, _, line):    \n",
    "    splits = line.rstrip(\"\\n\").split(\"|\")\n",
    "    \n",
    "    if len(splits) == 2: # country data\n",
    "      symbol = 'A' # make country sort before person data\n",
    "      country2digit = splits[1]\n",
    "      yield country2digit, [symbol, splits]\n",
    "    else: # person data\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      yield country2digit, [symbol, splits]\n",
    "  \n",
    "  def reducer(self, key, values):\n",
    "    countries = [] # should come first, as they are sorted on artificia key 'A'\n",
    "    for value in values:\n",
    "      if value[0] == 'A':    #if we have multiple records on left/country side\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:   #take the crossproduct country X Person\n",
    "          yield key, country[1:] + value[1:]\n",
    "      \n",
    "if __name__ == '__main__':\n",
    "  MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "creating tmp directory /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/MRJoin.jshanahan.20160626.121350.077099\n",
      "writing to /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/MRJoin.jshanahan.20160626.121350.077099/step-0-mapper_part-00000\n",
      "writing to /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/MRJoin.jshanahan.20160626.121350.077099/step-0-mapper_part-00001\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/MRJoin.jshanahan.20160626.121350.077099/step-0-mapper-sorted\n",
      "> sort /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/MRJoin.jshanahan.20160626.121350.077099/step-0-mapper_part-00000 /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/MRJoin.jshanahan.20160626.121350.077099/step-0-mapper_part-00001\n",
      "writing to /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/MRJoin.jshanahan.20160626.121350.077099/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/MRJoin.jshanahan.20160626.121350.077099/step-0-reducer_part-00000 -> /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/MRJoin.jshanahan.20160626.121350.077099/output/part-00000\n",
      "Streaming final output from /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/MRJoin.jshanahan.20160626.121350.077099/output\n",
      "\"CA\"\t[[\"Canada\", \"CA\"], [\"Jon Sneed\", \"valued\", \"CA\"]]\n",
      "\"CA\"\t[[\"Canada\", \"CA\"], [\"Jon York\", \"valued\", \"CA\"]]\n",
      "\"CA\"\t[[\"Canada\", \"CA\"], [\"Sam Sneed\", \"valued\", \"CA\"]]\n",
      "\"CA\"\t[[\"Canada\", \"CA\"], [\"Yo Yo Ma\", \"not so good\", \"CA\"]]\n",
      "\"UK\"\t[[\"United Kingdom\", \"UK\"], [\"Alex Ball\", \"valued\", \"UK\"]]\n",
      "\"UK\"\t[[\"United Kingdom\", \"UK\"], [\"Arnold Wesise\", \"not so good\", \"UK\"]]\n",
      "\"US\"\t[[\"United States\", \"US\"], [\"Alice Bob\", \"not bad\", \"US\"]]\n",
      "\"US\"\t[[\"United States\", \"US\"], [\"Henry Bob\", \"not bad\", \"US\"]]\n",
      "removing tmp directory /var/folders/j4/95k348x940xcz40fkdmgy_n40000gn/T/MRJoin.jshanahan.20160626.121350.077099\n"
     ]
    }
   ],
   "source": [
    "!python Join/MRJoin.py Join/countries.dat Join/customers.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Useful  Links for Jupyter Notebooks <a name=\"0\"></a> \n",
    "[Back to Table of Contents](#TOC)\n",
    " \n",
    "Some useful links for editing Jupyter notebooks:\n",
    "\n",
    "http://blog.jupyter.org/2016/01/08/notebook-4-1-release/\n",
    "    \n",
    "https://sowingseasons.com/blog/reference/2016/01/jupyter-keyboard-shortcuts/23298516\n",
    "\n",
    "[Markdown examples ](http://nbviewer.jupyter.org/github/ipython/ipython/blob/1.x/examples/notebooks/Part%204%20-%20Markdown%20Cells.ipynb)\n",
    "\n",
    "~~~\n",
    "Command Mode (press ESC to enable)\n",
    "Enter : enter edit mode\n",
    "Shift-Enter : run cell, select below\n",
    "Ctrl-Enter : run cell\n",
    "Alt-Enter : run cell, insert below\n",
    "Y : to code\n",
    "M : to markdown\n",
    "R : to raw\n",
    "1 : to heading 1\n",
    "2 : to heading 2\n",
    "3 : to heading 3\n",
    "4 : to heading 4\n",
    "5 : to heading 5\n",
    "6 : to heading 6\n",
    "Up : select cell above\n",
    "K : select cell above\n",
    "Down : select cell below \n",
    "SHIFT Down:  select one *or more cells* (to move up or down within a notebook) \n",
    "J : select cell below\n",
    "A : insert cell above\n",
    "B : insert cell below\n",
    "X : cut selected cell\n",
    "C : copy selected cell\n",
    "Shift-V : paste cell above\n",
    "V : paste cell below\n",
    "Z : undo last cell deletion\n",
    "D,D : delete selected cell\n",
    "Shift-M : merge cell below\n",
    "S : Save and Checkpoint\n",
    "Ctrl-S : Save and Checkpoint\n",
    "L : toggle line numbers\n",
    "O : toggle output\n",
    "Shift-O : toggle output scrolling\n",
    "Esc : close pager\n",
    "Q : close pager\n",
    "H : show keyboard shortcut help dialog\n",
    "I,I : interrupt kernel\n",
    "0,0 : restart kernel\n",
    "Space : scroll down\n",
    "Shift-Space : scroll up\n",
    "Shift : ignore\n",
    "Edit Mode (press Enter to enable)\n",
    "Tab : code completion or indent\n",
    "Shift-Tab : tooltip\n",
    "Ctrl-] : indent\n",
    "Ctrl-[ : dedent\n",
    "Ctrl-A : select all\n",
    "Ctrl-Z : undo\n",
    "Ctrl-Shift-Z : redo\n",
    "Ctrl-Y : redo\n",
    "Ctrl-Home : go to cell start\n",
    "Ctrl-Up : go to cell start\n",
    "Ctrl-End : go to cell end\n",
    "Ctrl-Down : go to cell end\n",
    "Ctrl-Left : go one word left\n",
    "Ctrl-Right : go one word right\n",
    "Ctrl-Backspace : delete word before\n",
    "Ctrl-Delete : delete word after\n",
    "Esc : command mode\n",
    "Ctrl-M : command mode\n",
    "Shift-Enter : run cell, select below\n",
    "Ctrl-Enter : run cell\n",
    "Alt-Enter : run cell, insert below\n",
    "Ctrl-Shift-Subtract : split cell\n",
    "Ctrl-Shift-- : split cell\n",
    "Ctrl-S : Save and Checkpoint\n",
    "Up : move cursor up or previous cell\n",
    "Down : move cursor down or next cell\n",
    "Shift : ignore\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -1.  End of Notebook <a name=\"0\"></a> \n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
