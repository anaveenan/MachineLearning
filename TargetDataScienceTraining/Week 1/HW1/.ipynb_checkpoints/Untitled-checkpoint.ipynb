{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile NaiveBayes/chineseExample.txt\n",
    "D1\t1\tChinese Beijing\tChinese\n",
    "D2\t1\tChinese Chinese\tShanghai\n",
    "D3\t1\tChinese\tMacao\n",
    "D4\t0\tTokyo Japan\tChinese\n",
    "D5\t0\tChinese Chinese\tChinese Tokyo Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile MRNaiveBayesTrainer.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRNaiveBayesTrainer(MRJob):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRNaiveBayesTrainer, self).__init__(*args, **kwargs)\n",
    "        self.modelStats = {}\n",
    "        classsTotalFreq = [0,0]\n",
    "\n",
    "    def jobconf(self):\n",
    "        orig_jobconf = super(MRNaiveBayesTrainer, self).jobconf()        \n",
    "        custom_jobconf = {\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1rn',\n",
    "            'mapred.reduce.tasks': '1',\n",
    "        }\n",
    "        combined_jobconf = orig_jobconf\n",
    "        combined_jobconf.update(custom_jobconf)\n",
    "        self.jobconf = combined_jobconf\n",
    "        return combined_jobconf\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        docID, docClass,text = line.split(\"\\t\",2)   \n",
    "        words = text.split()\n",
    "        if docID != \"D5\":\n",
    "            if docClass == \"1\":\n",
    "                yield \"PriorProbs\", \"0,1\"\n",
    "                yield \"!classTotalFreq\", \"0,\" + str(len(words))\n",
    "                for word in words:\n",
    "                    yield(word, \"0,1\")\n",
    "            else:\n",
    "                yield(\"PriorProbs\", \"1,0\")\n",
    "                yield \"!classTotalFreq\", str(len(words)) + \",0\"\n",
    "                for word in words:\n",
    "                    yield(word, \"1,0\")\n",
    "        \n",
    "#     def reducer(self, word, values):\n",
    "\n",
    "#         w0Total=0\n",
    "#         w1Total=0\n",
    "#         for value in values:\n",
    "#             w0, w1 =  value.split(\",\")\n",
    "#             w0Total += float(w0)\n",
    "#             w1Total += float(w1)  \n",
    "#         if word == \"!classTotalFreq\":\n",
    "#             self.modelStats[word] = [w0Total, w1Total]\n",
    "#             yield \n",
    "#         elif word == \"PriorProbs\":\n",
    "#             yield \"PriorProbs\", str(w0Total / (w0Total + w1Total))+\",\" + str(w1Total / (w0Total + w1Total))\n",
    "#         else:\n",
    "#             yield world, str(w0Total / self.modelStats[\"!classTotalFreq\"][0]) + \",\" + str(w1Total / self.modelStats[\"!classTotalFreq\"][1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "#         #yield(\"JIMI \"+word, [w0Total, w1Total])\n",
    "#     def reducer_final(self):       \n",
    "#         class0Total = 0\n",
    "#         class1Total = 0\n",
    "#         for k in self.modelStats.keys():\n",
    "#             if k != \"PriorProbs\":\n",
    "#                 class0Total += self.modelStats[k][0]\n",
    "#                 class1Total += self.modelStats[k][1]\n",
    "#         vocabularySize = len(self.modelStats.keys()) -1  #ignore TomsPriors\n",
    "#         #some yields to see some model internal parameters\n",
    "#         #yield (\"defaultPrior 0 class\", class0Total+vocabularySize)\n",
    "#         #yield (\"defaultPrior 1 class\", class1Total+vocabularySize)\n",
    "#         #yield (\"count 0 class\", class0Total)\n",
    "#         #yield (\"count 1 class\", class1Total)\n",
    "#         #yield (\"vocabularySize\", vocabularySize)\n",
    "        \n",
    "#         #calculate priors \n",
    "#         classCount0, classCount1 = self.modelStats.get(\"PriorProbs\")\n",
    "#         del self.modelStats[\"PriorProbs\"]\n",
    "#         total = classCount0 + classCount1\n",
    "#         yield(\"PriorProbs\", ','.join(str(j) for j in [classCount0, classCount1, classCount0/total, classCount1/total])) \n",
    "#         for k in self.modelStats.keys():\n",
    "#             yield(k, ','.join(str(j) for j in [self.modelStats[k][0],\n",
    "#                       self.modelStats[k][1],\n",
    "#                       (self.modelStats[k][0] + 1) /(class0Total + vocabularySize), \n",
    "#                       (self.modelStats[k][1] +1)/(class1Total+vocabularySize)]))        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRNaiveBayesTrainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python MRNaiveBayesTrainer.py --jobconf mapred.reduce.tasks=1 chineseExample.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
